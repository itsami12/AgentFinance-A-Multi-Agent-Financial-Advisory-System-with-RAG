{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0ddb4b8-baf4-478d-bfea-4a28e47516db",
   "metadata": {},
   "source": [
    "## 1. DATA ANALYST AGENT DATASETS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3947157-0f5f-415f-be19-b02b058b05d7",
   "metadata": {},
   "source": [
    "# Dataset 1: UCI Bank Marketing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4e808e0-9d68-4181-be01-867fe09f8789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>target_term_deposit</th>\n",
       "      <th>balance_bucket</th>\n",
       "      <th>duration_bucket</th>\n",
       "      <th>customer_profile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58</td>\n",
       "      <td>management</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>2143</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>261</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>medium</td>\n",
       "      <td>medium</td>\n",
       "      <td>58-year-old management customer who is married...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44</td>\n",
       "      <td>technician</td>\n",
       "      <td>single</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>29</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>151</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>low</td>\n",
       "      <td>medium</td>\n",
       "      <td>44-year-old technician customer who is single ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>entrepreneur</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>low</td>\n",
       "      <td>short</td>\n",
       "      <td>33-year-old entrepreneur customer who is marri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47</td>\n",
       "      <td>blue_collar</td>\n",
       "      <td>married</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>1506</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>92</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>medium</td>\n",
       "      <td>short</td>\n",
       "      <td>47-year-old blue_collar customer who is marrie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>unknown</td>\n",
       "      <td>single</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>198</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>low</td>\n",
       "      <td>medium</td>\n",
       "      <td>33-year-old unknown customer who is single wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45206</th>\n",
       "      <td>51</td>\n",
       "      <td>technician</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>825</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>17</td>\n",
       "      <td>nov</td>\n",
       "      <td>977</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "      <td>medium</td>\n",
       "      <td>long</td>\n",
       "      <td>51-year-old technician customer who is married...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45207</th>\n",
       "      <td>71</td>\n",
       "      <td>retired</td>\n",
       "      <td>divorced</td>\n",
       "      <td>primary</td>\n",
       "      <td>no</td>\n",
       "      <td>1729</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>17</td>\n",
       "      <td>nov</td>\n",
       "      <td>456</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>yes</td>\n",
       "      <td>medium</td>\n",
       "      <td>long</td>\n",
       "      <td>71-year-old retired customer who is divorced w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45208</th>\n",
       "      <td>72</td>\n",
       "      <td>retired</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>5715</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>17</td>\n",
       "      <td>nov</td>\n",
       "      <td>1127</td>\n",
       "      <td>5</td>\n",
       "      <td>184</td>\n",
       "      <td>3</td>\n",
       "      <td>success</td>\n",
       "      <td>yes</td>\n",
       "      <td>high</td>\n",
       "      <td>long</td>\n",
       "      <td>72-year-old retired customer who is married wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45209</th>\n",
       "      <td>57</td>\n",
       "      <td>blue_collar</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>668</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>17</td>\n",
       "      <td>nov</td>\n",
       "      <td>508</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>medium</td>\n",
       "      <td>long</td>\n",
       "      <td>57-year-old blue_collar customer who is marrie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45210</th>\n",
       "      <td>37</td>\n",
       "      <td>entrepreneur</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>2971</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>17</td>\n",
       "      <td>nov</td>\n",
       "      <td>361</td>\n",
       "      <td>2</td>\n",
       "      <td>188</td>\n",
       "      <td>11</td>\n",
       "      <td>other</td>\n",
       "      <td>no</td>\n",
       "      <td>high</td>\n",
       "      <td>long</td>\n",
       "      <td>37-year-old entrepreneur customer who is marri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45211 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age           job   marital  education default  balance housing loan  \\\n",
       "0       58    management   married   tertiary      no     2143     yes   no   \n",
       "1       44    technician    single  secondary      no       29     yes   no   \n",
       "2       33  entrepreneur   married  secondary      no        2     yes  yes   \n",
       "3       47   blue_collar   married    unknown      no     1506     yes   no   \n",
       "4       33       unknown    single    unknown      no        1      no   no   \n",
       "...    ...           ...       ...        ...     ...      ...     ...  ...   \n",
       "45206   51    technician   married   tertiary      no      825      no   no   \n",
       "45207   71       retired  divorced    primary      no     1729      no   no   \n",
       "45208   72       retired   married  secondary      no     5715      no   no   \n",
       "45209   57   blue_collar   married  secondary      no      668      no   no   \n",
       "45210   37  entrepreneur   married  secondary      no     2971      no   no   \n",
       "\n",
       "         contact day_of_week month  duration  campaign  pdays  previous  \\\n",
       "0        unknown           5   may       261         1      0         0   \n",
       "1        unknown           5   may       151         1      0         0   \n",
       "2        unknown           5   may        76         1      0         0   \n",
       "3        unknown           5   may        92         1      0         0   \n",
       "4        unknown           5   may       198         1      0         0   \n",
       "...          ...         ...   ...       ...       ...    ...       ...   \n",
       "45206   cellular          17   nov       977         3      0         0   \n",
       "45207   cellular          17   nov       456         2      0         0   \n",
       "45208   cellular          17   nov      1127         5    184         3   \n",
       "45209  telephone          17   nov       508         4      0         0   \n",
       "45210   cellular          17   nov       361         2    188        11   \n",
       "\n",
       "      poutcome target_term_deposit balance_bucket duration_bucket  \\\n",
       "0      unknown                  no         medium          medium   \n",
       "1      unknown                  no            low          medium   \n",
       "2      unknown                  no            low           short   \n",
       "3      unknown                  no         medium           short   \n",
       "4      unknown                  no            low          medium   \n",
       "...        ...                 ...            ...             ...   \n",
       "45206  unknown                 yes         medium            long   \n",
       "45207  unknown                 yes         medium            long   \n",
       "45208  success                 yes           high            long   \n",
       "45209  unknown                  no         medium            long   \n",
       "45210    other                  no           high            long   \n",
       "\n",
       "                                        customer_profile  \n",
       "0      58-year-old management customer who is married...  \n",
       "1      44-year-old technician customer who is single ...  \n",
       "2      33-year-old entrepreneur customer who is marri...  \n",
       "3      47-year-old blue_collar customer who is marrie...  \n",
       "4      33-year-old unknown customer who is single wit...  \n",
       "...                                                  ...  \n",
       "45206  51-year-old technician customer who is married...  \n",
       "45207  71-year-old retired customer who is divorced w...  \n",
       "45208  72-year-old retired customer who is married wi...  \n",
       "45209  57-year-old blue_collar customer who is marrie...  \n",
       "45210  37-year-old entrepreneur customer who is marri...  \n",
       "\n",
       "[45211 rows x 20 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# -----------------------------------\n",
    "# LOAD DATA\n",
    "# -----------------------------------\n",
    "bank_marketing = fetch_ucirepo(id=222)\n",
    "\n",
    "df = bank_marketing.data.features.copy()\n",
    "df[\"target_term_deposit\"] = bank_marketing.data.targets   # keep target with features\n",
    "\n",
    "\n",
    "# -----------------------------------\n",
    "# 1. CATEGORICAL CLEANING USING REGEX\n",
    "# -----------------------------------\n",
    "cat_cols = [\n",
    "    \"job\", \"marital\", \"education\", \"default\",\n",
    "    \"housing\", \"loan\", \"contact\", \"poutcome\",\n",
    "    \"day_of_week\", \"month\"\n",
    "]\n",
    "\n",
    "def regex_clean(x):\n",
    "    if pd.isna(x):\n",
    "        return \"unknown\"\n",
    "\n",
    "    x = str(x).lower()\n",
    "\n",
    "    # remove unwanted punctuation using regex\n",
    "    x = re.sub(r\"[^a-z0-9\\s\\-\\._]\", \"\", x)   # keep alphanumerics, dash, underscore, dot\n",
    "\n",
    "    # replace dash and dot with underscore using regex\n",
    "    x = re.sub(r\"[\\-\\.]\", \"_\", x)\n",
    "\n",
    "    # collapse multiple underscores\n",
    "    x = re.sub(r\"_+\", \"_\", x)\n",
    "\n",
    "    # strip whitespace/underscores\n",
    "    x = x.strip(\"_ \").replace(\" \", \"_\")\n",
    "\n",
    "    # if empty, mark unknown\n",
    "    return x if x else \"unknown\"\n",
    "\n",
    "for col in cat_cols:\n",
    "    df[col] = df[col].apply(regex_clean)\n",
    "\n",
    "\n",
    "# -----------------------------------\n",
    "# 2. HANDLE NUMERIC MISSING VALUES\n",
    "# -----------------------------------\n",
    "num_cols = [\"age\", \"balance\", \"duration\", \"campaign\", \"pdays\", \"previous\"]\n",
    "df[num_cols] = df[num_cols].fillna(0)\n",
    "\n",
    "# convert -1 in pdays to 0 (meaning \"not contacted before\")\n",
    "df[\"pdays\"] = df[\"pdays\"].replace(-1, 0)\n",
    "\n",
    "\n",
    "# -----------------------------------\n",
    "# 3. CLEAN YES/NO FIELDS USING REGEX\n",
    "# -----------------------------------\n",
    "yn_cols = [\"default\", \"housing\", \"loan\", \"target_term_deposit\"]\n",
    "\n",
    "for col in yn_cols:\n",
    "    df[col] = df[col].astype(str).str.lower()\n",
    "    df[col] = df[col].str.replace(r\"[^a-z]\", \"\", regex=True)  # remove unexpected chars\n",
    "    df[col] = df[col].replace({\"yes\": \"yes\", \"no\": \"no\"})\n",
    "    df[col] = df[col].apply(lambda x: \"no\" if x not in [\"yes\", \"no\"] else x)\n",
    "\n",
    "\n",
    "# -----------------------------------\n",
    "# 4. BUCKETIZE FINANCIAL COLUMNS (optional)\n",
    "# -----------------------------------\n",
    "def bucket_balance(x):\n",
    "    if x < 0: return \"negative\"\n",
    "    elif x < 500: return \"low\"\n",
    "    elif x < 2500: return \"medium\"\n",
    "    return \"high\"\n",
    "\n",
    "df[\"balance_bucket\"] = df[\"balance\"].apply(bucket_balance)\n",
    "\n",
    "def bucket_duration(x):\n",
    "    if x < 100: return \"short\"\n",
    "    elif x < 300: return \"medium\"\n",
    "    return \"long\"\n",
    "\n",
    "df[\"duration_bucket\"] = df[\"duration\"].apply(bucket_duration)\n",
    "\n",
    "\n",
    "# -----------------------------------\n",
    "# 5. NATURAL LANGUAGE PROFILE (optional)\n",
    "# -----------------------------------\n",
    "def build_profile(row):\n",
    "    return (\n",
    "        f\"{row['age']}-year-old {row['job']} customer \"\n",
    "        f\"who is {row['marital']} with {row['education']} education. \"\n",
    "        f\"Balance: {row['balance_bucket']}. Housing: {row['housing']}, loan: {row['loan']}. \"\n",
    "        f\"Contacted in {row['month']} ({row['day_of_week']}). \"\n",
    "        f\"Call duration: {row['duration']} sec. \"\n",
    "        f\"Previous outcome: {row['poutcome']}. \"\n",
    "        f\"Subscribed: {row['target_term_deposit']}.\"\n",
    "    )\n",
    "\n",
    "df[\"customer_profile\"] = df.apply(build_profile, axis=1)\n",
    "\n",
    "df.to_csv('UCI Bank Marketing Dataset.csv)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06867fe-ddfc-459e-a467-2409316398f7",
   "metadata": {},
   "source": [
    "# Dataset 2: UCI Adult Census Income Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7f8103f5-d1f6-402c-81e6-c3bffe8003fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "      <th>profile_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>state_gov</td>\n",
       "      <td>0.044131</td>\n",
       "      <td>bachelors</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>never_married</td>\n",
       "      <td>adm_clerical</td>\n",
       "      <td>not_in_family</td>\n",
       "      <td>white</td>\n",
       "      <td>male</td>\n",
       "      <td>0.021740</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>united_states</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>39-year-old male white individual with bachelo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>self_emp_not_inc</td>\n",
       "      <td>0.048052</td>\n",
       "      <td>bachelors</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>married_civ_spouse</td>\n",
       "      <td>exec_managerial</td>\n",
       "      <td>husband</td>\n",
       "      <td>white</td>\n",
       "      <td>male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>united_states</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>50-year-old male white individual with bachelo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>private</td>\n",
       "      <td>0.137581</td>\n",
       "      <td>hs_grad</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>divorced</td>\n",
       "      <td>handlers_cleaners</td>\n",
       "      <td>not_in_family</td>\n",
       "      <td>white</td>\n",
       "      <td>male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>united_states</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>38-year-old male white individual with hs_grad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>private</td>\n",
       "      <td>0.150486</td>\n",
       "      <td>11th</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>married_civ_spouse</td>\n",
       "      <td>handlers_cleaners</td>\n",
       "      <td>husband</td>\n",
       "      <td>black</td>\n",
       "      <td>male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>united_states</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>53-year-old male black individual with 11th ed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>private</td>\n",
       "      <td>0.220635</td>\n",
       "      <td>bachelors</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>married_civ_spouse</td>\n",
       "      <td>prof_specialty</td>\n",
       "      <td>wife</td>\n",
       "      <td>black</td>\n",
       "      <td>female</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>28-year-old female black individual with bache...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48837</th>\n",
       "      <td>39</td>\n",
       "      <td>private</td>\n",
       "      <td>0.137428</td>\n",
       "      <td>bachelors</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>divorced</td>\n",
       "      <td>prof_specialty</td>\n",
       "      <td>not_in_family</td>\n",
       "      <td>white</td>\n",
       "      <td>female</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>united_states</td>\n",
       "      <td>&lt;=50K.</td>\n",
       "      <td>39-year-old female white individual with bache...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48838</th>\n",
       "      <td>64</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0.209130</td>\n",
       "      <td>hs_grad</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>widowed</td>\n",
       "      <td>unknown</td>\n",
       "      <td>other_relative</td>\n",
       "      <td>black</td>\n",
       "      <td>male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>united_states</td>\n",
       "      <td>&lt;=50K.</td>\n",
       "      <td>64-year-old male black individual with hs_grad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48839</th>\n",
       "      <td>38</td>\n",
       "      <td>private</td>\n",
       "      <td>0.245379</td>\n",
       "      <td>bachelors</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>married_civ_spouse</td>\n",
       "      <td>prof_specialty</td>\n",
       "      <td>husband</td>\n",
       "      <td>white</td>\n",
       "      <td>male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>united_states</td>\n",
       "      <td>&lt;=50K.</td>\n",
       "      <td>38-year-old male white individual with bachelo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48840</th>\n",
       "      <td>44</td>\n",
       "      <td>private</td>\n",
       "      <td>0.048444</td>\n",
       "      <td>bachelors</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>divorced</td>\n",
       "      <td>adm_clerical</td>\n",
       "      <td>own_child</td>\n",
       "      <td>asian_pac_islander</td>\n",
       "      <td>male</td>\n",
       "      <td>0.054551</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>united_states</td>\n",
       "      <td>&lt;=50K.</td>\n",
       "      <td>44-year-old male asian_pac_islander individual...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48841</th>\n",
       "      <td>35</td>\n",
       "      <td>self_emp_inc</td>\n",
       "      <td>0.114919</td>\n",
       "      <td>bachelors</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>married_civ_spouse</td>\n",
       "      <td>exec_managerial</td>\n",
       "      <td>husband</td>\n",
       "      <td>white</td>\n",
       "      <td>male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.602041</td>\n",
       "      <td>united_states</td>\n",
       "      <td>&gt;50K.</td>\n",
       "      <td>35-year-old male white individual with bachelo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48842 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age         workclass    fnlwgt  education  education-num  \\\n",
       "0       39         state_gov  0.044131  bachelors       0.800000   \n",
       "1       50  self_emp_not_inc  0.048052  bachelors       0.800000   \n",
       "2       38           private  0.137581    hs_grad       0.533333   \n",
       "3       53           private  0.150486       11th       0.400000   \n",
       "4       28           private  0.220635  bachelors       0.800000   \n",
       "...    ...               ...       ...        ...            ...   \n",
       "48837   39           private  0.137428  bachelors       0.800000   \n",
       "48838   64           unknown  0.209130    hs_grad       0.533333   \n",
       "48839   38           private  0.245379  bachelors       0.800000   \n",
       "48840   44           private  0.048444  bachelors       0.800000   \n",
       "48841   35      self_emp_inc  0.114919  bachelors       0.800000   \n",
       "\n",
       "           marital-status         occupation    relationship  \\\n",
       "0           never_married       adm_clerical   not_in_family   \n",
       "1      married_civ_spouse    exec_managerial         husband   \n",
       "2                divorced  handlers_cleaners   not_in_family   \n",
       "3      married_civ_spouse  handlers_cleaners         husband   \n",
       "4      married_civ_spouse     prof_specialty            wife   \n",
       "...                   ...                ...             ...   \n",
       "48837            divorced     prof_specialty   not_in_family   \n",
       "48838             widowed            unknown  other_relative   \n",
       "48839  married_civ_spouse     prof_specialty         husband   \n",
       "48840            divorced       adm_clerical       own_child   \n",
       "48841  married_civ_spouse    exec_managerial         husband   \n",
       "\n",
       "                     race     sex  capital-gain  capital-loss  hours-per-week  \\\n",
       "0                   white    male      0.021740           0.0        0.397959   \n",
       "1                   white    male      0.000000           0.0        0.122449   \n",
       "2                   white    male      0.000000           0.0        0.397959   \n",
       "3                   black    male      0.000000           0.0        0.397959   \n",
       "4                   black  female      0.000000           0.0        0.397959   \n",
       "...                   ...     ...           ...           ...             ...   \n",
       "48837               white  female      0.000000           0.0        0.357143   \n",
       "48838               black    male      0.000000           0.0        0.397959   \n",
       "48839               white    male      0.000000           0.0        0.500000   \n",
       "48840  asian_pac_islander    male      0.054551           0.0        0.397959   \n",
       "48841               white    male      0.000000           0.0        0.602041   \n",
       "\n",
       "      native-country  income  \\\n",
       "0      united_states   <=50K   \n",
       "1      united_states   <=50K   \n",
       "2      united_states   <=50K   \n",
       "3      united_states   <=50K   \n",
       "4               cuba   <=50K   \n",
       "...              ...     ...   \n",
       "48837  united_states  <=50K.   \n",
       "48838  united_states  <=50K.   \n",
       "48839  united_states  <=50K.   \n",
       "48840  united_states  <=50K.   \n",
       "48841  united_states   >50K.   \n",
       "\n",
       "                                            profile_text  \n",
       "0      39-year-old male white individual with bachelo...  \n",
       "1      50-year-old male white individual with bachelo...  \n",
       "2      38-year-old male white individual with hs_grad...  \n",
       "3      53-year-old male black individual with 11th ed...  \n",
       "4      28-year-old female black individual with bache...  \n",
       "...                                                  ...  \n",
       "48837  39-year-old female white individual with bache...  \n",
       "48838  64-year-old male black individual with hs_grad...  \n",
       "48839  38-year-old male white individual with bachelo...  \n",
       "48840  44-year-old male asian_pac_islander individual...  \n",
       "48841  35-year-old male white individual with bachelo...  \n",
       "\n",
       "[48842 rows x 16 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# -----------------------------------\n",
    "# 1. LOAD DATA\n",
    "# -----------------------------------\n",
    "adult = fetch_ucirepo(id=2)\n",
    "\n",
    "df = adult.data.features.copy()\n",
    "df[\"income\"] = adult.data.targets  # include target column\n",
    "\n",
    "# -----------------------------------\n",
    "# 2. HANDLE MISSING VALUES\n",
    "# -----------------------------------\n",
    "# In Adult dataset, missing values are represented by '?'\n",
    "df.replace(\"?\", \"unknown\", inplace=True)\n",
    "\n",
    "# -----------------------------------\n",
    "# 3. CLEAN CATEGORICAL VARIABLES USING REGEX\n",
    "# -----------------------------------\n",
    "cat_cols = [\n",
    "    \"workclass\", \"education\", \"marital-status\",\n",
    "    \"occupation\", \"relationship\", \"race\", \"sex\", \"native-country\"\n",
    "]\n",
    "\n",
    "def regex_clean(x):\n",
    "    if pd.isna(x) or x == \"unknown\":\n",
    "        return \"unknown\"\n",
    "    x = str(x).lower()\n",
    "    x = re.sub(r\"[^a-z0-9\\s\\-\\._]\", \"\", x)  # remove unwanted chars\n",
    "    x = re.sub(r\"[\\-\\.]\", \"_\", x)           # replace dash/dot with underscore\n",
    "    x = re.sub(r\"_+\", \"_\", x)               # collapse multiple underscores\n",
    "    x = x.strip(\"_ \").replace(\" \", \"_\")     # clean edges and spaces\n",
    "    return x if x else \"unknown\"\n",
    "\n",
    "for col in cat_cols:\n",
    "    df[col] = df[col].apply(regex_clean)\n",
    "\n",
    "# -----------------------------------\n",
    "# 4. SCALE SELECT NUMERIC FEATURES (leave age raw)\n",
    "# -----------------------------------\n",
    "num_cols_to_scale = [\"fnlwgt\", \"education-num\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df[num_cols_to_scale] = scaler.fit_transform(df[num_cols_to_scale])\n",
    "\n",
    "# -----------------------------------\n",
    "# 5. OPTIONAL: BUILD NATURAL-LANGUAGE PROFILE\n",
    "# -----------------------------------\n",
    "def build_profile(row):\n",
    "    return (\n",
    "        f\"{row['age']}-year-old {row['sex']} {row['race']} individual \"\n",
    "        f\"with {row['education']} education, working as {row['occupation']} \"\n",
    "        f\"in {row['workclass']} sector. Marital status: {row['marital-status']}, \"\n",
    "        f\"relationship: {row['relationship']}. \"\n",
    "        f\"Capital gain: {row['capital-gain']:.2f}, capital loss: {row['capital-loss']:.2f}, \"\n",
    "        f\"works {row['hours-per-week']:.2f} hours per week. \"\n",
    "        f\"From {row['native-country']}. \"\n",
    "        f\"Income target: {row['income']}.\"\n",
    "    )\n",
    "\n",
    "df[\"profile_text\"] = df.apply(build_profile, axis=1)\n",
    "\n",
    "# -----------------------------------\n",
    "# 6. SAVE CLEANED DATA\n",
    "# -----------------------------------\n",
    "df.to_csv('UCI_Adult_Census_Income_Preprocessed.csv', index=False)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bc6b66-bbb8-4cb6-b4c4-6017028c8d6b",
   "metadata": {},
   "source": [
    "# Dataset 3: Kaggle Credit Card Fraud Detection Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b73e7175-7df9-43d9-a5de-1ca537367d0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction_time</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_22</th>\n",
       "      <th>feature_23</th>\n",
       "      <th>feature_24</th>\n",
       "      <th>feature_25</th>\n",
       "      <th>feature_26</th>\n",
       "      <th>feature_27</th>\n",
       "      <th>feature_28</th>\n",
       "      <th>transaction_amount</th>\n",
       "      <th>fraud_label</th>\n",
       "      <th>transaction_profile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>406.0</td>\n",
       "      <td>0.919012</td>\n",
       "      <td>0.787855</td>\n",
       "      <td>0.809517</td>\n",
       "      <td>0.429154</td>\n",
       "      <td>0.762201</td>\n",
       "      <td>0.248677</td>\n",
       "      <td>0.249897</td>\n",
       "      <td>0.800314</td>\n",
       "      <td>0.367355</td>\n",
       "      <td>...</td>\n",
       "      <td>0.508396</td>\n",
       "      <td>0.658525</td>\n",
       "      <td>0.425381</td>\n",
       "      <td>0.580406</td>\n",
       "      <td>0.454498</td>\n",
       "      <td>0.421331</td>\n",
       "      <td>0.310216</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>fraud</td>\n",
       "      <td>Transaction at 406.0 seconds since start, amou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>472.0</td>\n",
       "      <td>0.906588</td>\n",
       "      <td>0.733944</td>\n",
       "      <td>0.856275</td>\n",
       "      <td>0.353384</td>\n",
       "      <td>0.774870</td>\n",
       "      <td>0.252314</td>\n",
       "      <td>0.267339</td>\n",
       "      <td>0.784658</td>\n",
       "      <td>0.453446</td>\n",
       "      <td>...</td>\n",
       "      <td>0.530346</td>\n",
       "      <td>0.685868</td>\n",
       "      <td>0.342644</td>\n",
       "      <td>0.593612</td>\n",
       "      <td>0.401704</td>\n",
       "      <td>0.411845</td>\n",
       "      <td>0.313850</td>\n",
       "      <td>0.020591</td>\n",
       "      <td>fraud</td>\n",
       "      <td>Transaction at 472.0 seconds since start, amou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4462.0</td>\n",
       "      <td>0.919163</td>\n",
       "      <td>0.785821</td>\n",
       "      <td>0.831180</td>\n",
       "      <td>0.355228</td>\n",
       "      <td>0.760185</td>\n",
       "      <td>0.262258</td>\n",
       "      <td>0.268781</td>\n",
       "      <td>0.781104</td>\n",
       "      <td>0.454573</td>\n",
       "      <td>...</td>\n",
       "      <td>0.466535</td>\n",
       "      <td>0.667999</td>\n",
       "      <td>0.370467</td>\n",
       "      <td>0.569143</td>\n",
       "      <td>0.336811</td>\n",
       "      <td>0.417241</td>\n",
       "      <td>0.310018</td>\n",
       "      <td>0.009339</td>\n",
       "      <td>fraud</td>\n",
       "      <td>Transaction at 4462.0 seconds since start, amo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6986.0</td>\n",
       "      <td>0.883578</td>\n",
       "      <td>0.781591</td>\n",
       "      <td>0.792483</td>\n",
       "      <td>0.370723</td>\n",
       "      <td>0.758122</td>\n",
       "      <td>0.245862</td>\n",
       "      <td>0.244056</td>\n",
       "      <td>0.782717</td>\n",
       "      <td>0.454245</td>\n",
       "      <td>...</td>\n",
       "      <td>0.518287</td>\n",
       "      <td>0.658956</td>\n",
       "      <td>0.375025</td>\n",
       "      <td>0.592075</td>\n",
       "      <td>0.318049</td>\n",
       "      <td>0.401244</td>\n",
       "      <td>0.330364</td>\n",
       "      <td>0.002297</td>\n",
       "      <td>fraud</td>\n",
       "      <td>Transaction at 6986.0 seconds since start, amo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7519.0</td>\n",
       "      <td>0.979262</td>\n",
       "      <td>0.799121</td>\n",
       "      <td>0.762821</td>\n",
       "      <td>0.461731</td>\n",
       "      <td>0.790114</td>\n",
       "      <td>0.249369</td>\n",
       "      <td>0.275794</td>\n",
       "      <td>0.780061</td>\n",
       "      <td>0.418588</td>\n",
       "      <td>...</td>\n",
       "      <td>0.477181</td>\n",
       "      <td>0.655679</td>\n",
       "      <td>0.162235</td>\n",
       "      <td>0.661482</td>\n",
       "      <td>0.518034</td>\n",
       "      <td>0.416326</td>\n",
       "      <td>0.316103</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>fraud</td>\n",
       "      <td>Transaction at 7519.0 seconds since start, amo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>76376.0</td>\n",
       "      <td>0.932369</td>\n",
       "      <td>0.775660</td>\n",
       "      <td>0.857731</td>\n",
       "      <td>0.209946</td>\n",
       "      <td>0.766235</td>\n",
       "      <td>0.246374</td>\n",
       "      <td>0.269442</td>\n",
       "      <td>0.785939</td>\n",
       "      <td>0.432222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.506651</td>\n",
       "      <td>0.659535</td>\n",
       "      <td>0.493484</td>\n",
       "      <td>0.600636</td>\n",
       "      <td>0.576360</td>\n",
       "      <td>0.411670</td>\n",
       "      <td>0.310417</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>non-fraud</td>\n",
       "      <td>Transaction at 76376.0 seconds since start, am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>60348.0</td>\n",
       "      <td>0.946303</td>\n",
       "      <td>0.782221</td>\n",
       "      <td>0.854634</td>\n",
       "      <td>0.292210</td>\n",
       "      <td>0.764564</td>\n",
       "      <td>0.257320</td>\n",
       "      <td>0.267594</td>\n",
       "      <td>0.789911</td>\n",
       "      <td>0.437408</td>\n",
       "      <td>...</td>\n",
       "      <td>0.540624</td>\n",
       "      <td>0.664711</td>\n",
       "      <td>0.438161</td>\n",
       "      <td>0.561757</td>\n",
       "      <td>0.375551</td>\n",
       "      <td>0.422253</td>\n",
       "      <td>0.316410</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>non-fraud</td>\n",
       "      <td>Transaction at 60348.0 seconds since start, am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>47151.0</td>\n",
       "      <td>0.975201</td>\n",
       "      <td>0.765699</td>\n",
       "      <td>0.835526</td>\n",
       "      <td>0.303727</td>\n",
       "      <td>0.767936</td>\n",
       "      <td>0.274905</td>\n",
       "      <td>0.263430</td>\n",
       "      <td>0.788851</td>\n",
       "      <td>0.481417</td>\n",
       "      <td>...</td>\n",
       "      <td>0.510296</td>\n",
       "      <td>0.659926</td>\n",
       "      <td>0.195473</td>\n",
       "      <td>0.619785</td>\n",
       "      <td>0.394194</td>\n",
       "      <td>0.417811</td>\n",
       "      <td>0.313965</td>\n",
       "      <td>0.003850</td>\n",
       "      <td>non-fraud</td>\n",
       "      <td>Transaction at 47151.0 seconds since start, am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>53857.0</td>\n",
       "      <td>0.937946</td>\n",
       "      <td>0.800673</td>\n",
       "      <td>0.777166</td>\n",
       "      <td>0.288225</td>\n",
       "      <td>0.774539</td>\n",
       "      <td>0.258545</td>\n",
       "      <td>0.267696</td>\n",
       "      <td>0.790726</td>\n",
       "      <td>0.481826</td>\n",
       "      <td>...</td>\n",
       "      <td>0.497670</td>\n",
       "      <td>0.666776</td>\n",
       "      <td>0.154349</td>\n",
       "      <td>0.579755</td>\n",
       "      <td>0.378800</td>\n",
       "      <td>0.430398</td>\n",
       "      <td>0.320137</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>non-fraud</td>\n",
       "      <td>Transaction at 53857.0 seconds since start, am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>65405.0</td>\n",
       "      <td>0.948797</td>\n",
       "      <td>0.762706</td>\n",
       "      <td>0.870229</td>\n",
       "      <td>0.172889</td>\n",
       "      <td>0.753918</td>\n",
       "      <td>0.268275</td>\n",
       "      <td>0.272399</td>\n",
       "      <td>0.780770</td>\n",
       "      <td>0.523504</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555194</td>\n",
       "      <td>0.660194</td>\n",
       "      <td>0.388947</td>\n",
       "      <td>0.596394</td>\n",
       "      <td>0.358068</td>\n",
       "      <td>0.413678</td>\n",
       "      <td>0.304118</td>\n",
       "      <td>0.010951</td>\n",
       "      <td>non-fraud</td>\n",
       "      <td>Transaction at 65405.0 seconds since start, am...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>984 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     transaction_time  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "0               406.0   0.919012   0.787855   0.809517   0.429154   0.762201   \n",
       "1               472.0   0.906588   0.733944   0.856275   0.353384   0.774870   \n",
       "2              4462.0   0.919163   0.785821   0.831180   0.355228   0.760185   \n",
       "3              6986.0   0.883578   0.781591   0.792483   0.370723   0.758122   \n",
       "4              7519.0   0.979262   0.799121   0.762821   0.461731   0.790114   \n",
       "..                ...        ...        ...        ...        ...        ...   \n",
       "979           76376.0   0.932369   0.775660   0.857731   0.209946   0.766235   \n",
       "980           60348.0   0.946303   0.782221   0.854634   0.292210   0.764564   \n",
       "981           47151.0   0.975201   0.765699   0.835526   0.303727   0.767936   \n",
       "982           53857.0   0.937946   0.800673   0.777166   0.288225   0.774539   \n",
       "983           65405.0   0.948797   0.762706   0.870229   0.172889   0.753918   \n",
       "\n",
       "     feature_6  feature_7  feature_8  feature_9  ...  feature_22  feature_23  \\\n",
       "0     0.248677   0.249897   0.800314   0.367355  ...    0.508396    0.658525   \n",
       "1     0.252314   0.267339   0.784658   0.453446  ...    0.530346    0.685868   \n",
       "2     0.262258   0.268781   0.781104   0.454573  ...    0.466535    0.667999   \n",
       "3     0.245862   0.244056   0.782717   0.454245  ...    0.518287    0.658956   \n",
       "4     0.249369   0.275794   0.780061   0.418588  ...    0.477181    0.655679   \n",
       "..         ...        ...        ...        ...  ...         ...         ...   \n",
       "979   0.246374   0.269442   0.785939   0.432222  ...    0.506651    0.659535   \n",
       "980   0.257320   0.267594   0.789911   0.437408  ...    0.540624    0.664711   \n",
       "981   0.274905   0.263430   0.788851   0.481417  ...    0.510296    0.659926   \n",
       "982   0.258545   0.267696   0.790726   0.481826  ...    0.497670    0.666776   \n",
       "983   0.268275   0.272399   0.780770   0.523504  ...    0.555194    0.660194   \n",
       "\n",
       "     feature_24  feature_25  feature_26  feature_27  feature_28  \\\n",
       "0      0.425381    0.580406    0.454498    0.421331    0.310216   \n",
       "1      0.342644    0.593612    0.401704    0.411845    0.313850   \n",
       "2      0.370467    0.569143    0.336811    0.417241    0.310018   \n",
       "3      0.375025    0.592075    0.318049    0.401244    0.330364   \n",
       "4      0.162235    0.661482    0.518034    0.416326    0.316103   \n",
       "..          ...         ...         ...         ...         ...   \n",
       "979    0.493484    0.600636    0.576360    0.411670    0.310417   \n",
       "980    0.438161    0.561757    0.375551    0.422253    0.316410   \n",
       "981    0.195473    0.619785    0.394194    0.417811    0.313965   \n",
       "982    0.154349    0.579755    0.378800    0.430398    0.320137   \n",
       "983    0.388947    0.596394    0.358068    0.413678    0.304118   \n",
       "\n",
       "     transaction_amount  fraud_label  \\\n",
       "0              0.000000        fraud   \n",
       "1              0.020591        fraud   \n",
       "2              0.009339        fraud   \n",
       "3              0.002297        fraud   \n",
       "4              0.000039        fraud   \n",
       "..                  ...          ...   \n",
       "979            0.000895    non-fraud   \n",
       "980            0.000109    non-fraud   \n",
       "981            0.003850    non-fraud   \n",
       "982            0.000035    non-fraud   \n",
       "983            0.010951    non-fraud   \n",
       "\n",
       "                                   transaction_profile  \n",
       "0    Transaction at 406.0 seconds since start, amou...  \n",
       "1    Transaction at 472.0 seconds since start, amou...  \n",
       "2    Transaction at 4462.0 seconds since start, amo...  \n",
       "3    Transaction at 6986.0 seconds since start, amo...  \n",
       "4    Transaction at 7519.0 seconds since start, amo...  \n",
       "..                                                 ...  \n",
       "979  Transaction at 76376.0 seconds since start, am...  \n",
       "980  Transaction at 60348.0 seconds since start, am...  \n",
       "981  Transaction at 47151.0 seconds since start, am...  \n",
       "982  Transaction at 53857.0 seconds since start, am...  \n",
       "983  Transaction at 65405.0 seconds since start, am...  \n",
       "\n",
       "[984 rows x 32 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 1. LOAD DATA\n",
    "df = pd.read_csv(\"creditcard.csv\")\n",
    "\n",
    "# 2. RENAME COLUMNS\n",
    "new_col_names = {\n",
    "    'Time': 'transaction_time',\n",
    "    'Amount': 'transaction_amount',\n",
    "    'Class': 'fraud_label'\n",
    "}\n",
    "\n",
    "for i in range(1, 29):\n",
    "    new_col_names[f'V{i}'] = f'feature_{i}'\n",
    "\n",
    "df.rename(columns=new_col_names, inplace=True)\n",
    "\n",
    "# 3. CONVERT TARGET\n",
    "df['fraud_label'] = df['fraud_label'].apply(lambda x: 'fraud' if x == 1 else 'non-fraud')\n",
    "\n",
    "# 4. MIN-MAX SCALE PCA FEATURES TO [0,1]\n",
    "feature_cols = [f'feature_{i}' for i in range(1, 29)]\n",
    "scaler = MinMaxScaler()\n",
    "df[feature_cols] = scaler.fit_transform(df[feature_cols])\n",
    "\n",
    "# Optionally scale transaction_amount too\n",
    "df['transaction_amount'] = MinMaxScaler().fit_transform(df[['transaction_amount']])\n",
    "\n",
    "# 5. OPTIONAL: NATURAL-LANGUAGE SUMMARY\n",
    "def build_profile(row):\n",
    "    features = ', '.join([f\"{row[f]:.3f}\" for f in feature_cols])\n",
    "    return (\n",
    "        f\"Transaction at {row['transaction_time']} seconds since start, \"\n",
    "        f\"amount {row['transaction_amount']:.2f} (normalized). \"\n",
    "        f\"Features: {features}. \"\n",
    "        f\"Fraud label: {row['fraud_label']}.\"\n",
    "    )\n",
    "\n",
    "df['transaction_profile'] = df.apply(build_profile, axis=1)\n",
    "\n",
    "# 6. OPTIONAL: STRATIFIED SAMPLING FOR BALANCED DATA\n",
    "fraud_df = df[df['fraud_label'] == 'fraud']\n",
    "non_fraud_df = df[df['fraud_label'] == 'non-fraud'].sample(n=len(fraud_df), random_state=42)\n",
    "df_sample = pd.concat([fraud_df, non_fraud_df]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "df_sample.to_csv('Kaggle Credit Card Fraud Detection Dataset.csv')\n",
    "df_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fce3a3-5285-48c2-ae91-2010c8281289",
   "metadata": {},
   "source": [
    "# Dataset 4: Consumer Expenditure Survey (BLS - US Bureau of Labor Statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "415b0d5f-9022-483e-a046-697b8934ecee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing FMLI files ---\n",
      "Successfully loaded: intrvw23\\intrvw23\\fmli232.csv (4,751 rows)\n",
      "Successfully loaded: intrvw23\\intrvw23\\fmli233.csv (4,770 rows)\n",
      "Successfully loaded: intrvw23\\intrvw23\\fmli234.csv (4,662 rows)\n",
      "Successfully loaded: intrvw23\\intrvw23\\fmli241.csv (4,688 rows)\n",
      "Combined 4 files into 18,871 total rows for FMLI\n",
      "\n",
      "--- Processing ITBI files ---\n",
      "Successfully loaded: intrvw23\\intrvw23\\itbi232.csv (271,662 rows)\n",
      "Successfully loaded: intrvw23\\intrvw23\\itbi233.csv (272,544 rows)\n",
      "Successfully loaded: intrvw23\\intrvw23\\itbi234.csv (265,401 rows)\n",
      "Successfully loaded: intrvw23\\intrvw23\\itbi241.csv (268,224 rows)\n",
      "Combined 4 files into 1,077,831 total rows for ITBI\n",
      "\n",
      "--- Processing ITII files ---\n",
      "Successfully loaded: intrvw23\\intrvw23\\itii232.csv (330,450 rows)\n",
      "Successfully loaded: intrvw23\\intrvw23\\itii233.csv (330,840 rows)\n",
      "Successfully loaded: intrvw23\\intrvw23\\itii234.csv (322,320 rows)\n",
      "Successfully loaded: intrvw23\\intrvw23\\itii241.csv (325,200 rows)\n",
      "Combined 4 files into 1,308,810 total rows for ITII\n",
      "\n",
      "--- Aggregating ITBI Data (1,077,831 rows) ---\n",
      "Aggregated ITBI data reduced to 18,871 rows (one row per consumer unit/month).\n",
      "\n",
      "--- Aggregating ITII Data (1,308,810 rows) ---\n",
      "Aggregated ITII data reduced to 18,841 rows (one row per consumer unit/month).\n",
      "\n",
      "--- Starting Merging of Aggregated Data (Left Join on FMLI) ---\n",
      "Current DataFrame size after merge: 18,871 rows.\n",
      "Current DataFrame size after merge: 18,871 rows.\n",
      "\n",
      "--- Final Dataset Summary ---\n",
      "Master CEX DataFrame shape: (18871, 904)\n",
      "First 5 rows of the Master DataFrame:\n",
      "     NEWID  DIRACC DIRACC_  AGE_REF AGE_REF_  AGE2 AGE2_  AS_COMP1 AS_C_MP1  \\\n",
      "0  5090604     1.0       D       74        D  72.0     D         1        D   \n",
      "1  5090624     1.0       D       87        T   0.0     A         0        D   \n",
      "2  5090634     1.0       D       58        D   0.0     A         0        D   \n",
      "3  5090664     1.0       D       55        D  52.0     D         1        D   \n",
      "4  5090674     1.0       D       31        D  31.0     D         1        D   \n",
      "\n",
      "   AS_COMP2  ... ITII_900140  ITII_900150 ITII_900160  ITII_900170  \\\n",
      "0         1  ...         0.0          0.0         0.0      3236.25   \n",
      "1         1  ...         0.0          0.0         0.0         0.00   \n",
      "2         1  ...         0.0          0.0         0.0         0.00   \n",
      "3         1  ...         0.0          0.0         0.0         0.00   \n",
      "4         2  ...         0.0          0.0         0.0         0.00   \n",
      "\n",
      "  ITII_900180  ITII_900190 ITII_900200  ITII_900210  ITII_980000  \\\n",
      "0     62.5005          0.0         0.0          0.0   50846.2500   \n",
      "1      0.0000          0.0         0.0          0.0   17233.7505   \n",
      "2      0.0000          0.0         0.0          0.0  168750.0000   \n",
      "3      0.0000          0.0         0.0          0.0   78716.7501   \n",
      "4      0.0000          0.0         0.0          0.0   46774.9995   \n",
      "\n",
      "     ITII_980071  \n",
      "0   50846.250000  \n",
      "1   17233.750001  \n",
      "2  133823.749999  \n",
      "3   72415.500000  \n",
      "4   45593.750000  \n",
      "\n",
      "[5 rows x 904 columns]\n",
      "\n",
      "Successfully saved the complete dataset to: cex_master_dataset_for_rag.csv\n",
      "\n",
      "--- NOTE ON UNPROCESSED FILES ---\n",
      "The following files were found in your folders but NOT combined or merged, as they do not match the core CEX patterns ('fmli', 'itbi', 'itii'):\n",
      "- creditcard.csv\n",
      "- Kaggle Credit Card Fraud Detection Dataset.csv\n",
      "- UCI Bank Marketing Dataset.csv\n",
      "- UCI_Adult_Census_Income_Preprocessed.csv\n",
      "- crisis-sim\\crisis-sim\\eval\\results\\agg\\all_results.csv\n",
      "- crisis-sim\\crisis-sim\\eval\\results\\agg\\summary.csv\n",
      "- intrvw23\\intrvw23\\memi232.csv\n",
      "- intrvw23\\intrvw23\\memi233.csv\n",
      "- intrvw23\\intrvw23\\memi234.csv\n",
      "- intrvw23\\intrvw23\\memi241.csv\n",
      "- intrvw23\\intrvw23\\mtbi232.csv\n",
      "- intrvw23\\intrvw23\\mtbi233.csv\n",
      "- intrvw23\\intrvw23\\mtbi234.csv\n",
      "- intrvw23\\intrvw23\\mtbi241.csv\n",
      "- intrvw23\\intrvw23\\ntaxi232.csv\n",
      "- intrvw23\\intrvw23\\ntaxi233.csv\n",
      "- intrvw23\\intrvw23\\ntaxi234.csv\n",
      "- intrvw23\\intrvw23\\ntaxi241.csv\n",
      "- intrvw23\\intrvw23\\expn23\\apa23.csv\n",
      "- intrvw23\\intrvw23\\expn23\\apb23.csv\n",
      "- intrvw23\\intrvw23\\expn23\\cld23.csv\n",
      "- intrvw23\\intrvw23\\expn23\\cle23.csv\n",
      "- intrvw23\\intrvw23\\expn23\\cnt23.csv\n",
      "- intrvw23\\intrvw23\\expn23\\cra23.csv\n",
      "- intrvw23\\intrvw23\\expn23\\crb23.csv\n",
      "- intrvw23\\intrvw23\\expn23\\eqb23.csv\n",
      "- intrvw23\\intrvw23\\expn23\\exp23.csv\n",
      "- intrvw23\\intrvw23\\expn23\\fce23.csv\n",
      "- intrvw23\\intrvw23\\expn23\\fra23.csv\n",
      "- intrvw23\\intrvw23\\expn23\\frb23.csv\n",
      "- intrvw23\\intrvw23\\expn23\\hel23.csv\n",
      "- intrvw23\\intrvw23\\expn23\\hhm23.csv\n",
      "- intrvw23\\intrvw23\\expn23\\hhp23.csv\n",
      "- intrvw23\\intrvw23\\expn23\\him23.csv\n",
      "- intrvw23\\intrvw23\\expn23\\inb23.csv\n",
      "- intrvw23\\intrvw23\\expn23\\lsd23.csv\n",
      "- intrvw23\\intrvw23\\expn23\\mdb23.csv\n",
      "- intrvw23\\intrvw23\\expn23\\mdc23.csv\n",
      "- intrvw23\\intrvw23\\expn23\\mis23.csv\n",
      "- intrvw23\\intrvw23\\expn23\\mor23.csv\n",
      "- intrvw23\\intrvw23\\expn23\\opb23.csv\n",
      "- intrvw23\\intrvw23\\expn23\\opd23.csv\n",
      "- intrvw23\\intrvw23\\expn23\\oph23.csv\n",
      "- intrvw23\\intrvw23\\expn23\\opi23.csv\n",
      "- intrvw23\\intrvw23\\expn23\\ovb23.csv\n",
      "- intrvw23\\intrvw23\\expn23\\ovc23.csv\n",
      "- intrvw23\\intrvw23\\expn23\\pbt23.csv\n",
      "- intrvw23\\intrvw23\\expn23\\rnt23.csv\n",
      "- intrvw23\\intrvw23\\expn23\\rtv23.csv\n",
      "- intrvw23\\intrvw23\\expn23\\sub23.csv\n",
      "- intrvw23\\intrvw23\\expn23\\trd23.csv\n",
      "- intrvw23\\intrvw23\\expn23\\tre23.csv\n",
      "- intrvw23\\intrvw23\\expn23\\trf23.csv\n",
      "- intrvw23\\intrvw23\\expn23\\trv23.csv\n",
      "- intrvw23\\intrvw23\\expn23\\uta23.csv\n",
      "- intrvw23\\intrvw23\\expn23\\utc23.csv\n",
      "- intrvw23\\intrvw23\\expn23\\utp23.csv\n",
      "- intrvw23\\intrvw23\\expn23\\vot23.csv\n",
      "- intrvw23\\intrvw23\\expn23\\vqb23.csv\n",
      "- intrvw23\\intrvw23\\expn23\\vtr23.csv\n",
      "- intrvw23\\intrvw23\\expn23\\xpb23.csv\n",
      "- intrvw23\\intrvw23\\para23\\fpar2223.csv\n",
      "- intrvw23\\intrvw23\\para23\\mchi2223.csv\n",
      "- jigsaw-agile-community-rules\\sample_submission.csv\n",
      "- jigsaw-agile-community-rules\\test.csv\n",
      "- jigsaw-agile-community-rules\\train.csv\n",
      "- lab_knowledge_ops_complete\\data\\manifest.csv\n",
      "- lab_knowledge_ops_complete\\data\\tenant_acl.csv\n",
      "\n",
      "These are likely supplementary files (e.g., codebooks, lookup tables) and may need custom loading/merging depending on your RAG strategy.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "from functools import reduce\n",
    "\n",
    "# --- Configuration ---\n",
    "# The primary keys used for merging the different CEX record types.\n",
    "# NEWID is the unique consumer unit ID. REFYR/REFMO specifies the interview year/month.\n",
    "MERGE_KEYS = 'NEWID'\n",
    "\n",
    "# Find all CSV files recursively in the current directory and all subdirectories.\n",
    "# This ensures we catch files regardless of which subfolder they are in.\n",
    "try:\n",
    "    ALL_FILES = glob.glob('**/*.csv', recursive=True)\n",
    "    if not ALL_FILES:\n",
    "        print(\"Error: No CSV files found. Ensure the script is running from inside the main data folder.\")\n",
    "        sys.exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during file search: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "def load_and_combine_data(file_prefix):\n",
    "    \"\"\"\n",
    "    Finds all CSV files matching a prefix (e.g., 'fmli'), loads them,\n",
    "    and combines them into a single DataFrame.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Processing {file_prefix.upper()} files ---\")\n",
    "    \n",
    "    # Filter the master list of ALL_FILES based on the file prefix\n",
    "    files_to_process = [f for f in ALL_FILES if os.path.basename(f).startswith(file_prefix)]\n",
    "    \n",
    "    if not files_to_process:\n",
    "        print(f\"No files found for prefix: {file_prefix}. Skipping combination for this type.\")\n",
    "        return None\n",
    "\n",
    "    df_list = []\n",
    "    \n",
    "    for file in files_to_process:\n",
    "        try:\n",
    "            # low_memory=False is crucial for CEX data due to inconsistent column data types\n",
    "            # Only load necessary columns for ITBI/ITII to avoid memory overflow on large transaction files\n",
    "            if file_prefix in ['itbi', 'itii']:\n",
    "                 # We only need the keys, the UCC code (the type of item), and the VALUE amount.\n",
    "                 df = pd.read_csv(file, low_memory=False, usecols=['NEWID', 'REFYR', 'REFMO', 'UCC', 'VALUE'])\n",
    "            else:\n",
    "                df = pd.read_csv(file, low_memory=False)\n",
    "\n",
    "            df_list.append(df)\n",
    "            print(f\"Successfully loaded: {file} ({len(df):,} rows)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file}: {e}. Skipping file. Details: {e}\")\n",
    "            \n",
    "    if df_list:\n",
    "        combined_df = pd.concat(df_list, ignore_index=True)\n",
    "        print(f\"Combined {len(df_list)} files into {len(combined_df):,} total rows for {file_prefix.upper()}\")\n",
    "        \n",
    "        # Standardize merge keys to string type to prevent merge errors\n",
    "        for col in MERGE_KEYS:\n",
    "            if col in combined_df.columns:\n",
    "                combined_df[col] = combined_df[col].astype(str)\n",
    "        \n",
    "        return combined_df\n",
    "    return None\n",
    "\n",
    "def aggregate_transaction_data(df, prefix):\n",
    "    \"\"\"\n",
    "    Aggregates transaction-level data (ITBI or ITII) by summing the 'VALUE' \n",
    "    for each unique NEWID/REFYR/REFMO and UCC code.\n",
    "    \n",
    "    This turns millions of transaction rows into a condensed table \n",
    "    that can be merged without MemoryError.\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\n--- Aggregating {prefix.upper()} Data ({len(df):,} rows) ---\")\n",
    "    \n",
    "    # Use pivot_table to quickly aggregate:\n",
    "    # Rows: Unique Consumer Unit (defined by MERGE_KEYS)\n",
    "    # Columns: Unique UCC codes (each UCC is a spending/income category)\n",
    "    # Values: Sum of the VALUE (dollar amount)\n",
    "    try:\n",
    "        agg_df = df.pivot_table(\n",
    "            index=MERGE_KEYS,\n",
    "            columns='UCC',\n",
    "            values='VALUE',\n",
    "            aggfunc='sum',\n",
    "            fill_value=0 # Fill missing transactions with 0\n",
    "        ).reset_index()\n",
    "    except Exception as e:\n",
    "        # Fallback for extreme memory pressure, though pivot_table is usually the best approach\n",
    "        print(f\"Fatal Error during pivot_table aggregation: {e}\")\n",
    "        print(\"Please consider running this script on a machine with more RAM.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    \n",
    "    # --- FIX for TypeError: 'in <string>' requires string as left operand, not int ---\n",
    "    # Convert all column names to strings before comparison/renaming\n",
    "    agg_df.columns = agg_df.columns.astype(str)\n",
    "    \n",
    "    # Rename columns to identify source (e.g., UCC_000100 becomes ITBI_000100)\n",
    "    agg_df.columns = [f'{prefix.upper()}_{col}' if col not in MERGE_KEYS else col for col in agg_df.columns]\n",
    "    \n",
    "    print(f\"Aggregated {prefix.upper()} data reduced to {len(agg_df):,} rows (one row per consumer unit/month).\")\n",
    "    return agg_df\n",
    "\n",
    "def preprocess_and_merge(fmli_df, itbi_agg_df, itii_agg_df):\n",
    "    \"\"\"\n",
    "    Merges the FMLI base with the aggregated transaction data.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting Merging of Aggregated Data (Left Join on FMLI) ---\")\n",
    "\n",
    "    # List of DataFrames to merge, starting with the base (FMLI)\n",
    "    dfs_to_merge = [itbi_agg_df, itii_agg_df]\n",
    "    \n",
    "    final_cex_df = fmli_df\n",
    "    \n",
    "    for df in dfs_to_merge:\n",
    "        if df is not None:\n",
    "            # We use a Left Join to ensure every Consumer Unit (from FMLI) is kept\n",
    "            final_cex_df = pd.merge(\n",
    "                final_cex_df,\n",
    "                df,\n",
    "                on=MERGE_KEYS,\n",
    "                how='left'\n",
    "            )\n",
    "            print(f\"Current DataFrame size after merge: {len(final_cex_df):,} rows.\")\n",
    "        else:\n",
    "            print(f\"Skipping merge (DF is None).\")\n",
    "\n",
    "    # Fill NaN values (where a Consumer Unit had no transactions for a merged UCC code) with 0\n",
    "    final_cex_df = final_cex_df.fillna(0) \n",
    "\n",
    "    return final_cex_df\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "# Step 1: Combine files by type\n",
    "fmli_df = load_and_combine_data('fmli')\n",
    "itbi_df = load_and_combine_data('itbi')\n",
    "itii_df = load_and_combine_data('itii')\n",
    "\n",
    "# Step 2: Ensure the core demographic file (fmli) is available to proceed\n",
    "if fmli_df is None:\n",
    "    print(\"\\nFatal Error: The core FMLI (Family/Demographics) file(s) could not be loaded. Cannot proceed with merging.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Step 3: Aggregate transaction files (ITBI and ITII) to prevent MemoryError\n",
    "itbi_agg_df = aggregate_transaction_data(itbi_df, 'itbi')\n",
    "itii_agg_df = aggregate_transaction_data(itii_df, 'itii')\n",
    "\n",
    "\n",
    "# Step 4: Merge the combined and aggregated DataFrames\n",
    "cex_master_df = preprocess_and_merge(fmli_df, itbi_agg_df, itii_agg_df)\n",
    "\n",
    "# Step 5: Final inspection and saving\n",
    "print(\"\\n--- Final Dataset Summary ---\")\n",
    "print(f\"Master CEX DataFrame shape: {cex_master_df.shape}\")\n",
    "print(\"First 5 rows of the Master DataFrame:\")\n",
    "print(cex_master_df.head())\n",
    "\n",
    "# Save the final dataset to a single CSV file, ready for RAG indexing\n",
    "output_filename = 'cex_master_dataset_for_rag.csv'\n",
    "cex_master_df.to_csv(output_filename, index=False)\n",
    "print(f\"\\nSuccessfully saved the complete dataset to: {output_filename}\")\n",
    "\n",
    "# --- Handling Other Files (Identification) ---\n",
    "# This part helps you identify the other files you mentioned\n",
    "processed_files = []\n",
    "for prefix in ['fmli', 'itbi', 'itii']:\n",
    "    processed_files.extend([f for f in ALL_FILES if os.path.basename(f).startswith(prefix)])\n",
    "\n",
    "# Find files in ALL_FILES that were NOT in the processed list\n",
    "unprocessed_files = [f for f in ALL_FILES if f not in processed_files]\n",
    "\n",
    "if unprocessed_files:\n",
    "    print(\"\\n--- NOTE ON UNPROCESSED FILES ---\")\n",
    "    print(\"The following files were found in your folders but NOT combined or merged, as they do not match the core CEX patterns ('fmli', 'itbi', 'itii'):\")\n",
    "    for f in unprocessed_files:\n",
    "        print(f\"- {f}\")\n",
    "    print(\"\\nThese are likely supplementary files (e.g., codebooks, lookup tables) and may need custom loading/merging depending on your RAG strategy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a55fa6-8114-4d76-b61f-dc94647ceb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# --- Configuration ---\n",
    "INPUT_FILENAME = 'cex_master_dataset_for_rag.csv'\n",
    "OUTPUT_FILENAME = 'cex_master_clean.csv'\n",
    "\n",
    "# --- 1. Define the Column Renaming Map ---\n",
    "# Map the original CEX codes (FMLI/ITBI/ITII) to descriptive, RAG-friendly names.\n",
    "RENAMING_MAP = {\n",
    "    # --- Identifiers & Weight ---\n",
    "    'NEWID': 'Consumer_Unit_ID',\n",
    "    'REFYR': 'Reference_Year',\n",
    "    'REFMO': 'Reference_Month',\n",
    "    'FINLWT21': 'Final_Interview_Weight_Factor',\n",
    "    \n",
    "    # --- Demographics (FMLI) ---\n",
    "    'FAM_SIZE': 'Family_Size',\n",
    "    'AGE_REF': 'Age_Reference_Person',\n",
    "    'EDUC_REF': 'Education_Reference_Person',\n",
    "    'CUTENURE': 'Homeownership_Status', # 1=Owned, 2=Rented, 3=Occupied without payment\n",
    "    'REGION': 'Geographic_Region', # 1=NE, 2=MW, 3=S, 4=W\n",
    "    'BLS_URBN': 'BLS_Urban_Rural_Status',\n",
    "    'WORKCOMP': 'Employment_Status_Reference_Person',\n",
    "    \n",
    "    # --- Total Income & Spending (FMLI) ---\n",
    "    'FINCBTAX': 'Total_Income_Before_Tax',\n",
    "    'FSALARYX': 'Total_Salary_Wage_Income',\n",
    "    'FSSIX': 'Total_Social_Security_Income',\n",
    "    \n",
    "    # --- Aggregated Expenditure Categories (ITBI_) ---\n",
    "    # NOTE: You must consult the CEX UCC Codebook for the full list.\n",
    "    # We include key examples here:\n",
    "    'ITBI_005100': 'EXP_Food_At_Home',\n",
    "    'ITBI_005110': 'EXP_Food_Away_From_Home',\n",
    "    'ITBI_900000': 'EXP_Shelter_Rent_or_Mortgage',\n",
    "    'ITBI_480500': 'EXP_Gasoline_And_Motor_Oil',\n",
    "    'ITBI_550000': 'EXP_Vehicle_Purchases_Net',\n",
    "    'ITBI_210000': 'EXP_Apparel_and_Services',\n",
    "    'ITBI_320100': 'EXP_Health_Insurance_Premiums',\n",
    "    \n",
    "    # --- Aggregated Income Categories (ITII_) ---\n",
    "    'ITII_900030': 'INC_Wages_And_Salaries',\n",
    "    'ITII_900140': 'INC_Social_Security',\n",
    "    'ITII_900150': 'INC_Public_Assistance_Welfare',\n",
    "}\n",
    "\n",
    "def rename_and_save_data():\n",
    "    \"\"\"\n",
    "    Loads the merged CEX file, renames columns, and saves the cleaned file.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(INPUT_FILENAME):\n",
    "        print(f\"Error: Input file '{INPUT_FILENAME}' not found.\")\n",
    "        print(\"Please ensure you run the 'data_integration.py' script successfully first.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    print(f\"Loading merged dataset: {INPUT_FILENAME}...\")\n",
    "    try:\n",
    "        # Load the large file\n",
    "        df = pd.read_csv(INPUT_FILENAME, low_memory=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load the CSV: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    print(f\"DataFrame loaded with {len(df.columns)} columns.\")\n",
    "\n",
    "    # --- 2. Rename Columns ---\n",
    "    # Filter the renaming map to only include columns that actually exist in the DataFrame\n",
    "    valid_renaming_map = {\n",
    "        old_name: new_name \n",
    "        for old_name, new_name in RENAMING_MAP.items() \n",
    "        if old_name in df.columns\n",
    "    }\n",
    "    \n",
    "    # Identify and apply a generalized rename to the remaining UCC codes\n",
    "    # This prevents hundreds of unreadable UCC codes from remaining.\n",
    "    generic_rename_map = {}\n",
    "    for col in df.columns:\n",
    "        if col.startswith('ITBI_') and col not in valid_renaming_map:\n",
    "            generic_rename_map[col] = f'EXP_UCC_{col[5:]}'\n",
    "        elif col.startswith('ITII_') and col not in valid_renaming_map:\n",
    "            generic_rename_map[col] = f'INC_UCC_{col[5:]}'\n",
    "\n",
    "    final_renaming_map = {**valid_renaming_map, **generic_rename_map}\n",
    "    \n",
    "    # Apply the renaming\n",
    "    df.rename(columns=final_renaming_map, inplace=True)\n",
    "    \n",
    "    print(f\"Successfully renamed {len(final_renaming_map)} columns.\")\n",
    "    \n",
    "    # --- 3. Save the Cleaned File ---\n",
    "    print(f\"Saving cleaned dataset to: {OUTPUT_FILENAME}...\")\n",
    "    df.to_csv(OUTPUT_FILENAME, index=False)\n",
    "    print(f\"Success! Cleaned dataset saved. Total columns: {len(df.columns)}.\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    rename_and_save_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e97bb1-1c5f-439b-9783-bc143811397f",
   "metadata": {},
   "source": [
    "#  Dataset 5: FRED Economic Data (Federal Reserve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f40552ad-ff1b-4b14-85f7-2df56a1a5ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\itsam\\AppData\\Local\\Temp\\ipykernel_10924\\3335750587.py:49: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_clean = df.fillna(method='ffill').fillna(method='bfill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Stationarity Check (ADF Test) ===\n",
      "Consumer_Price_Index: ADF Statistic=3.197, p-value=1.000\n",
      "Unemployment_Rate: ADF Statistic=-3.587, p-value=0.006\n",
      "Federal_Funds_Rate: ADF Statistic=-2.295, p-value=0.174\n",
      "Personal_Income: ADF Statistic=5.504, p-value=1.000\n",
      "Consumer_Confidence_Index: ADF Statistic=-3.849, p-value=0.002\n",
      "House_Price_Index: ADF Statistic=2.482, p-value=0.999\n",
      "\n",
      "✅ Preprocessing complete!\n",
      "Raw data saved to: fred_macro_data_raw.csv\n",
      "Processed data saved to: fred_macro_data_processed.csv\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# FRED Macro Data Preprocessing\n",
    "# ================================\n",
    "\n",
    "from fredapi import Fred\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# -------------------------\n",
    "# 1. Initialize FRED API\n",
    "# -------------------------\n",
    "api_key = '7242fe69a51188cad3114480ea35cf15'\n",
    "fred = Fred(api_key=api_key)\n",
    "\n",
    "# -------------------------\n",
    "# 2. Define series mapping\n",
    "# -------------------------\n",
    "series_mapping = {\n",
    "    'CPIAUCNS': 'Consumer_Price_Index',         # CPI, All Urban Consumers\n",
    "    'UNRATE': 'Unemployment_Rate',             # Unemployment Rate\n",
    "    'FEDFUNDS': 'Federal_Funds_Rate',          # Federal Funds Rate\n",
    "    'W875RX1': 'Personal_Income',              # Personal Income\n",
    "    'UMCSENT': 'Consumer_Confidence_Index',    # Michigan Consumer Sentiment\n",
    "    'CSUSHPISA': 'House_Price_Index'           # S&P/Case-Shiller House Price Index\n",
    "}\n",
    "\n",
    "# -------------------------\n",
    "# 3. Fetch series\n",
    "# -------------------------\n",
    "data = {}\n",
    "for series_id, friendly_name in series_mapping.items():\n",
    "    try:\n",
    "        data[friendly_name] = fred.get_series(series_id)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {series_id}: {e}\")\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.index.name = 'Date'\n",
    "\n",
    "# Save raw dataset\n",
    "df.to_csv('fred_macro_data_raw.csv')\n",
    "\n",
    "# -------------------------\n",
    "# 4. Handle NaN values\n",
    "# -------------------------\n",
    "# Forward fill then backward fill\n",
    "df_clean = df.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# -------------------------\n",
    "# 5. Stationarity check\n",
    "# -------------------------\n",
    "non_stationary_cols = []\n",
    "\n",
    "print(\"\\n=== Stationarity Check (ADF Test) ===\")\n",
    "for col in df_clean.columns:\n",
    "    result = adfuller(df_clean[col].dropna())\n",
    "    p_value = result[1]\n",
    "    if p_value >= 0.05:\n",
    "        non_stationary_cols.append(col)\n",
    "    print(f\"{col}: ADF Statistic={result[0]:.3f}, p-value={p_value:.3f}\")\n",
    "\n",
    "# -------------------------\n",
    "# 6. Transform non-stationary series\n",
    "# -------------------------\n",
    "# Use log difference (log-return) to make stationary\n",
    "df_stationary = df_clean.copy()\n",
    "for col in non_stationary_cols:\n",
    "    # Avoid log(0)\n",
    "    df_stationary[col] = np.log(df_clean[col].replace(0, np.nan)).diff()\n",
    "\n",
    "df_stationary = df_stationary.dropna()\n",
    "\n",
    "# -------------------------\n",
    "# 7. Seasonal adjustment (optional example for CPI)\n",
    "# -------------------------\n",
    "if 'Consumer_Price_Index' in df_stationary.columns:\n",
    "    result = seasonal_decompose(df_clean['Consumer_Price_Index'], model='additive', period=12)\n",
    "    df_stationary['CPI_seasonally_adjusted'] = df_clean['Consumer_Price_Index'] - result.seasonal\n",
    "\n",
    "# -------------------------\n",
    "# 8. Save processed dataset\n",
    "# -------------------------\n",
    "df_stationary.to_csv('fred_macro_data_processed.csv')\n",
    "\n",
    "print(\"\\n✅ Preprocessing complete!\")\n",
    "print(\"Raw data saved to: fred_macro_data_raw.csv\")\n",
    "print(\"Processed data saved to: fred_macro_data_processed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42546bf-3ab2-4e6f-8a30-d23fc3bd9eac",
   "metadata": {},
   "source": [
    "# Dataset 6: World Bank Open Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f1dc2dc-0b4d-40af-9c42-e9858c4013d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  country       date           GDP  Population  Life_Expectancy  Poverty_Rate\n",
      "1   Aruba 2023-01-01  3.648573e+09    107359.0           76.353           NaN\n",
      "2   Aruba 2022-01-01  3.279344e+09    107310.0           76.226           NaN\n",
      "3   Aruba 2021-01-01  2.929447e+09    107700.0           73.655           NaN\n",
      "4   Aruba 2020-01-01  2.481857e+09    108587.0           75.406           NaN\n",
      "5   Aruba 2019-01-01  3.395799e+09    109203.0           76.019           NaN\n"
     ]
    }
   ],
   "source": [
    "import wbdata\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "# -------------------------\n",
    "# 1. Define indicators\n",
    "# -------------------------\n",
    "indicators = {\n",
    "    'NY.GDP.MKTP.CD': 'GDP',                   # GDP (current US$)\n",
    "    'SP.POP.TOTL': 'Population',               # Total population\n",
    "    'SP.DYN.LE00.IN': 'Life_Expectancy',       # Life expectancy at birth\n",
    "    'SI.POV.DDAY': 'Poverty_Rate'              # Poverty headcount ratio (%)\n",
    "}\n",
    "\n",
    "# -------------------------\n",
    "# 2. Get all country ISO codes\n",
    "# -------------------------\n",
    "all_countries = wbdata.get_countries()  # returns list of dicts\n",
    "countries = [c['id'] for c in all_countries if c['region']['id'] != 'NA']  # exclude aggregates\n",
    "\n",
    "# -------------------------\n",
    "# 3. Fetch data for all countries (no data_date)\n",
    "# -------------------------\n",
    "df = wbdata.get_dataframe(\n",
    "    indicators,\n",
    "    country=countries\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 4. Reset index\n",
    "# -------------------------\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "# -------------------------\n",
    "# 5. Filter by date range\n",
    "# -------------------------\n",
    "start_year = 1960\n",
    "end_year = 2023\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df[(df['date'].dt.year >= start_year) & (df['date'].dt.year <= end_year)]\n",
    "\n",
    "# -------------------------\n",
    "# 6. Preview and save\n",
    "# -------------------------\n",
    "print(df.head())\n",
    "df.to_csv('worldbank_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2c102c3-1f61-4ea3-b7af-7afb252768ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  country        date           GDP  Population  Life_Expectancy  Poverty_Rate\n",
      "0   Aruba  2023-01-01  3.648573e+09    107359.0           76.353           NaN\n",
      "1   Aruba  2022-01-01  3.279344e+09    107310.0           76.226           NaN\n",
      "2   Aruba  2021-01-01  2.929447e+09    107700.0           73.655           NaN\n",
      "3   Aruba  2020-01-01  2.481857e+09    108587.0           75.406           NaN\n",
      "4   Aruba  2019-01-01  3.395799e+09    109203.0           76.019           NaN\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13888 entries, 0 to 13887\n",
      "Data columns (total 6 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   country          13888 non-null  object \n",
      " 1   date             13888 non-null  object \n",
      " 2   GDP              11366 non-null  float64\n",
      " 3   Population       13858 non-null  float64\n",
      " 4   Life_Expectancy  13854 non-null  float64\n",
      " 5   Poverty_Rate     2389 non-null   float64\n",
      "dtypes: float64(4), object(2)\n",
      "memory usage: 651.1+ KB\n",
      "None\n",
      "         country        date       GDP  Population  Life_Expectancy  \\\n",
      "127  Afghanistan  1960-01-01  0.000127    0.006281         0.289323   \n",
      "126  Afghanistan  1961-01-01  0.000127    0.006405         0.295849   \n",
      "125  Afghanistan  1962-01-01  0.000127    0.006538         0.302031   \n",
      "124  Afghanistan  1963-01-01  0.000127    0.006677         0.307921   \n",
      "123  Afghanistan  1964-01-01  0.000127    0.006823         0.314182   \n",
      "\n",
      "     Poverty_Rate  \n",
      "127      0.030928  \n",
      "126      0.030928  \n",
      "125      0.030928  \n",
      "124      0.030928  \n",
      "123      0.030928  \n",
      "✅ Preprocessing complete. Saved to worldbank_data_processed.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\itsam\\AppData\\Local\\Temp\\ipykernel_10924\\14183729.py:19: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)\n",
      "C:\\Users\\itsam\\AppData\\Local\\Temp\\ipykernel_10924\\14183729.py:22: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='bfill', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# -------------------------\n",
    "# 1. Load CSV\n",
    "# -------------------------\n",
    "df = pd.read_csv('worldbank_data.csv')\n",
    "\n",
    "# Preview\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "\n",
    "# -------------------------\n",
    "# 2. Handle missing values\n",
    "# -------------------------\n",
    "# Option 1: Forward fill by country (use previous year)\n",
    "df.sort_values(by=['country', 'date'], inplace=True)\n",
    "df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Option 2: Backward fill by country\n",
    "df.fillna(method='bfill', inplace=True)\n",
    "\n",
    "# Option 3: If still missing, fill with overall mean\n",
    "df.fillna(df.mean(numeric_only=True), inplace=True)\n",
    "\n",
    "# -------------------------\n",
    "# 3. Handle different reporting standards\n",
    "# -------------------------\n",
    "# Example: Convert all GDP and income to USD if needed\n",
    "# For now, assume GDP is in current USD, so no conversion required\n",
    "# Otherwise, you would multiply by conversion rates for historical years\n",
    "\n",
    "# -------------------------\n",
    "# 4. Normalize numeric columns\n",
    "# -------------------------\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "scaler = MinMaxScaler()\n",
    "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "\n",
    "# -------------------------\n",
    "# 5. Preview processed data\n",
    "# -------------------------\n",
    "print(df.head())\n",
    "\n",
    "# -------------------------\n",
    "# 6. Save processed CSV\n",
    "# -------------------------\n",
    "df.to_csv('worldbank_data_processed.csv', index=False)\n",
    "print(\"✅ Preprocessing complete. Saved to worldbank_data_processed.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c689924b-a399-4e77-9797-8c72d98147a5",
   "metadata": {},
   "source": [
    "## 2. INVESTMENT ADVISOR AGENT DATASETS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30fa858-c084-4fed-8168-f0c65ea0c253",
   "metadata": {},
   "source": [
    "# Dataset 1: S&P 500 & NASDAQ Historical OHLCV Data (Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4eab3d93-8721-40b4-9e02-61f4f94767d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\itsam\\AppData\\Local\\Temp\\ipykernel_10924\\926265880.py:53: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data[numeric_cols] = data[numeric_cols].fillna(method='ffill').fillna(method='bfill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaned and saved to: C:\\Users\\itsam\\Downloads\\archive\\S&P500_NASDAQ_Cleaned.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open_Price</th>\n",
       "      <th>High_Price</th>\n",
       "      <th>Low_Price</th>\n",
       "      <th>Close_Price</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Change_Pct</th>\n",
       "      <th>Avg_Vol_20D</th>\n",
       "      <th>Adj_Close</th>\n",
       "      <th>Raw_Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1985-09-25</td>\n",
       "      <td>0.005792</td>\n",
       "      <td>0.005784</td>\n",
       "      <td>0.005831</td>\n",
       "      <td>0.005824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.509429</td>\n",
       "      <td>0.015504</td>\n",
       "      <td>0.052333</td>\n",
       "      <td>0.192496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1985-09-26</td>\n",
       "      <td>0.005763</td>\n",
       "      <td>0.005755</td>\n",
       "      <td>0.005801</td>\n",
       "      <td>0.005794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.509429</td>\n",
       "      <td>0.015504</td>\n",
       "      <td>0.052333</td>\n",
       "      <td>0.192496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1985-09-27</td>\n",
       "      <td>0.005763</td>\n",
       "      <td>0.005755</td>\n",
       "      <td>0.005801</td>\n",
       "      <td>0.005794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.521662</td>\n",
       "      <td>0.015504</td>\n",
       "      <td>0.052333</td>\n",
       "      <td>0.192496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1985-09-30</td>\n",
       "      <td>0.005762</td>\n",
       "      <td>0.005754</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.005793</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.521152</td>\n",
       "      <td>0.015504</td>\n",
       "      <td>0.052333</td>\n",
       "      <td>0.192496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1985-10-01</td>\n",
       "      <td>0.005762</td>\n",
       "      <td>0.005837</td>\n",
       "      <td>0.005797</td>\n",
       "      <td>0.005876</td>\n",
       "      <td>0.013179</td>\n",
       "      <td>0.556575</td>\n",
       "      <td>0.015504</td>\n",
       "      <td>0.052333</td>\n",
       "      <td>0.192496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  Open_Price  High_Price  Low_Price  Close_Price    Volume  \\\n",
       "0 1985-09-25    0.005792    0.005784   0.005831     0.005824  0.000000   \n",
       "1 1985-09-26    0.005763    0.005755   0.005801     0.005794  0.000000   \n",
       "2 1985-09-27    0.005763    0.005755   0.005801     0.005794  0.000000   \n",
       "3 1985-09-30    0.005762    0.005754   0.005800     0.005793  0.000000   \n",
       "4 1985-10-01    0.005762    0.005837   0.005797     0.005876  0.013179   \n",
       "\n",
       "   Change_Pct  Avg_Vol_20D  Adj_Close  Raw_Close  \n",
       "0    0.509429     0.015504   0.052333   0.192496  \n",
       "1    0.509429     0.015504   0.052333   0.192496  \n",
       "2    0.521662     0.015504   0.052333   0.192496  \n",
       "3    0.521152     0.015504   0.052333   0.192496  \n",
       "4    0.556575     0.015504   0.052333   0.192496  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# -------------------------\n",
    "# 1. Folder path & CSV files\n",
    "# -------------------------\n",
    "folder_path = r\"C:\\Users\\itsam\\Downloads\\archive\"\n",
    "csv_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "\n",
    "if not csv_files:\n",
    "    raise ValueError(\"No CSV files found in the folder. Check the path!\")\n",
    "\n",
    "# -------------------------\n",
    "# 2. Load and combine all CSVs\n",
    "# -------------------------\n",
    "df_list = []\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file, parse_dates=['date'])\n",
    "    df_list.append(df)\n",
    "\n",
    "data = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# -------------------------\n",
    "# 3. Rename columns for clarity\n",
    "# -------------------------\n",
    "rename_mapping = {\n",
    "    'date': 'Date',\n",
    "    'open': 'Open_Price',\n",
    "    'high': 'High_Price',\n",
    "    'low': 'Low_Price',\n",
    "    'close': 'Close_Price',\n",
    "    'adjusted_close': 'Adj_Close',\n",
    "    'volume': 'Volume',\n",
    "    'change_percent': 'Change_Pct',\n",
    "    'avg_vol_20d': 'Avg_Vol_20D',\n",
    "    'raw_close': 'Raw_Close'\n",
    "}\n",
    "\n",
    "existing_cols = {k: v for k, v in rename_mapping.items() if k in data.columns}\n",
    "data.rename(columns=existing_cols, inplace=True)\n",
    "\n",
    "# -------------------------\n",
    "# 4. Handle missing values\n",
    "# -------------------------\n",
    "numeric_cols = [col for col in ['Open_Price', 'High_Price', 'Low_Price',\n",
    "                                'Close_Price', 'Adj_Close', 'Volume',\n",
    "                                'Change_Pct', 'Avg_Vol_20D', 'Raw_Close'] \n",
    "                if col in data.columns]\n",
    "\n",
    "# Forward fill then backward fill\n",
    "data[numeric_cols] = data[numeric_cols].fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# Optional: If raw_close is still NaN, fill from Close_Price\n",
    "if 'Raw_Close' in data.columns:\n",
    "    data['Raw_Close'] = data['Raw_Close'].fillna(data['Close_Price'])\n",
    "\n",
    "# -------------------------\n",
    "# 5. Normalize numeric columns\n",
    "# -------------------------\n",
    "scaler = MinMaxScaler()\n",
    "data[numeric_cols] = scaler.fit_transform(data[numeric_cols])\n",
    "\n",
    "# -------------------------\n",
    "# 6. Save final cleaned dataset\n",
    "# -------------------------\n",
    "output_file = os.path.join(folder_path, \"S&P500_NASDAQ_Cleaned.csv\")\n",
    "data.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Data cleaned and saved to: {output_file}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b657c3-4014-49dd-83d2-835c24126351",
   "metadata": {},
   "source": [
    "#  Dataset 2: Yahoo Finance API (via yfinance Python Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "34444216-785a-4b8d-9f36-a3e86396d905",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tickers: 100%|███████████████████████████████████████████████████████████████| 5/5 [00:13<00:00,  2.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved concatenated data to 'yfinance_all_tickers.csv'\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# 1. Define tickers\n",
    "# -------------------------\n",
    "# Example: S&P 500 ETFs + index tickers\n",
    "tickers = [\"AAPL\", \"MSFT\", \"GOOG\", \"SPY\", \"QQQ\"]  # add more as needed\n",
    "\n",
    "# -------------------------\n",
    "# 2. Download data\n",
    "# -------------------------\n",
    "all_data = []\n",
    "\n",
    "for ticker_symbol in tqdm(tickers, desc=\"Downloading tickers\"):\n",
    "    try:\n",
    "        ticker = yf.Ticker(ticker_symbol)\n",
    "        \n",
    "        # Historical price + OHLCV data\n",
    "        hist = ticker.history(period=\"max\")  # fetch maximum available\n",
    "        \n",
    "        if hist.empty:\n",
    "            print(f\"Warning: No data for {ticker_symbol}\")\n",
    "            continue\n",
    "\n",
    "        # Reset index to make 'Date' a column\n",
    "        hist = hist.reset_index()\n",
    "        \n",
    "        # Add ticker column\n",
    "        hist['Ticker'] = ticker_symbol\n",
    "        \n",
    "        # Include dividends and splits as columns\n",
    "        hist['Dividends'] = hist['Dividends'].fillna(0)\n",
    "        hist['Stock_Splits'] = hist['Stock Splits'].fillna(0)\n",
    "        \n",
    "        # Append to list\n",
    "        all_data.append(hist)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {ticker_symbol}: {e}\")\n",
    "\n",
    "# -------------------------\n",
    "# 3. Concatenate all tickers\n",
    "# -------------------------\n",
    "if all_data:\n",
    "    df_all = pd.concat(all_data, axis=0, ignore_index=True)\n",
    "    \n",
    "    # Optional: sort by date and ticker\n",
    "    df_all = df_all.sort_values(by=['Ticker', 'Date']).reset_index(drop=True)\n",
    "    \n",
    "    # -------------------------\n",
    "    # 4. Save to CSV\n",
    "    # -------------------------\n",
    "    df_all.to_csv(\"yfinance_all_tickers.csv\", index=False)\n",
    "    print(\"Saved concatenated data to 'yfinance_all_tickers.csv'\")\n",
    "else:\n",
    "    print(\"No data downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ee0119f6-5a67-4e58-86c6-c97f4cb1ce2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from yfinance_all_tickers.csv...\n",
      "\n",
      "Starting Missing Value Imputation...\n",
      "Shape after imputation and initial cleaning: (41658, 11)\n",
      "Number of unique tickers: 5\n",
      "\n",
      "Audit: Found 18 historical dates with recorded Stock Splits.\n",
      "Latest trading date in dataset: 2025-11-20\n",
      "\n",
      "Identified 0 tickers as 'Inactive' (last trade before 2025-05-24):\n",
      "[]\n",
      "\n",
      "Shape after removing inactive/delisted stocks: (41658, 11)\n",
      "Removed 1 low-quality rows (Volume=0 or Close<=0).\n",
      "\n",
      "--- Preprocessing Summary ---\n",
      "Final dataset shape: (41657, 11)\n",
      "Final number of unique, active tickers: 5\n",
      "Cleaned data saved to preprocessed_stock_data.csv\n",
      "\n",
      "--- Snapshot of Preprocessed Data ---\n",
      "                 Date      Open      High       Low     Close     Volume  \\\n",
      "0 1980-12-12 05:00:00  0.098389  0.098817  0.098389  0.098389  469033600   \n",
      "1 1980-12-15 05:00:00  0.093684  0.093684  0.093256  0.093256  175884800   \n",
      "2 1980-12-16 05:00:00  0.086839  0.086839  0.086412  0.086412  105728000   \n",
      "3 1980-12-17 05:00:00  0.088550  0.088978  0.088550  0.088550   86441600   \n",
      "4 1980-12-18 05:00:00  0.091118  0.091545  0.091118  0.091118   73449600   \n",
      "\n",
      "   Dividends  Stock Splits Ticker  Stock_Splits  Capital Gains  \n",
      "0        0.0           0.0   AAPL           0.0            0.0  \n",
      "1        0.0           0.0   AAPL           0.0            0.0  \n",
      "2        0.0           0.0   AAPL           0.0            0.0  \n",
      "3        0.0           0.0   AAPL           0.0            0.0  \n",
      "4        0.0           0.0   AAPL           0.0            0.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# --- Configuration ---\n",
    "FILE_PATH = 'yfinance_all_tickers.csv'\n",
    "OUTPUT_FILE_PATH = 'preprocessed_stock_data.csv'\n",
    "# Define the threshold for considering a stock \"inactive\" (e.g., hasn't traded in the last 6 months)\n",
    "INACTIVE_THRESHOLD = 180 # days\n",
    "\n",
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads historical stock data and performs essential preprocessing steps,\n",
    "    including handling missing values.\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from {file_path}...\")\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return None\n",
    "\n",
    "    # 1. Initial Cleaning and Type Conversion\n",
    "    \n",
    "    # Ensure Date is in datetime format and set as index\n",
    "    # Note: The original Date column in the CSV may include timezone info, which we handle\n",
    "    df['Date'] = pd.to_datetime(df['Date'], utc=True).dt.tz_localize(None)\n",
    "    \n",
    "    # Convert core columns to numeric, coercing errors (useful for handling 'N/A' or bad data)\n",
    "    numeric_cols = ['Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits', 'Capital Gains']\n",
    "    for col in numeric_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        \n",
    "    # Identify categorical columns (primarily 'Ticker')\n",
    "    categorical_cols = ['Ticker']\n",
    "\n",
    "    # Drop rows where 'Ticker' is missing, as we cannot identify the security\n",
    "    df.dropna(subset=categorical_cols, inplace=True)\n",
    "    \n",
    "    # --- NEW IMPUTATION STEP ---\n",
    "    print(\"\\nStarting Missing Value Imputation...\")\n",
    "    \n",
    "    # Impute Numerical Columns: Forward Fill (FFill) is the best choice for time-series data, \n",
    "    # as we assume the price or volume from the previous day carries over if the current day's data is missing.\n",
    "    # Group by Ticker before applying FFill to ensure we don't use data from a different stock to fill gaps.\n",
    "    df = df.sort_values(by=['Ticker', 'Date']).set_index('Date')\n",
    "    df[numeric_cols] = df.groupby('Ticker')[numeric_cols].ffill()\n",
    "    df.reset_index(inplace=True)\n",
    "    \n",
    "    # After FFill, if the first few rows of a stock still have NaNs (no previous data to fill from),\n",
    "    # we can fill them with 0 for Dividends/Splits/Capital Gains, as a missing value implies 0 for these events.\n",
    "    # For price/volume, we will drop the remaining rows, as FFill should handle most cases.\n",
    "    df.dropna(subset=['Close'], inplace=True)\n",
    "    \n",
    "    # Since Dividends, Stock Splits, and Capital Gains should be 0 if missing, fill remaining NaNs with 0\n",
    "    df[['Dividends', 'Stock Splits', 'Capital Gains']] = df[['Dividends', 'Stock Splits', 'Capital Gains']].fillna(0)\n",
    "    \n",
    "    print(f\"Shape after imputation and initial cleaning: {df.shape}\")\n",
    "    print(f\"Number of unique tickers: {df['Ticker'].nunique()}\")\n",
    "    \n",
    "    # 2. Check and Account for Stock Splits (Already handled by data source, but audit remains)\n",
    "    split_count = df[df['Stock Splits'] != 0].shape[0]\n",
    "    print(f\"\\nAudit: Found {split_count} historical dates with recorded Stock Splits.\")\n",
    "    \n",
    "    # 3. Handle Delisted/Inactive Stocks\n",
    "    \n",
    "    latest_data_date = df['Date'].max()\n",
    "    print(f\"Latest trading date in dataset: {latest_data_date.date()}\")\n",
    "    \n",
    "    # Calculate the last recorded date for each ticker\n",
    "    last_dates = df.groupby('Ticker')['Date'].max()\n",
    "    \n",
    "    # Identify tickers whose last date is older than the INACTIVE_THRESHOLD\n",
    "    inactive_cutoff_date = latest_data_date - timedelta(days=INACTIVE_THRESHOLD)\n",
    "    inactive_tickers = last_dates[last_dates < inactive_cutoff_date].index.tolist()\n",
    "    \n",
    "    print(f\"\\nIdentified {len(inactive_tickers)} tickers as 'Inactive' (last trade before {inactive_cutoff_date.date()}):\")\n",
    "    # Display the first 10 inactive tickers for user inspection\n",
    "    print(inactive_tickers[:10]) \n",
    "\n",
    "    # Filter out inactive tickers\n",
    "    df_clean = df[~df['Ticker'].isin(inactive_tickers)].copy()\n",
    "    print(f\"\\nShape after removing inactive/delisted stocks: {df_clean.shape}\")\n",
    "    \n",
    "    # 4. Quality Filtering (Handling Micro-cap/Low-quality Data)\n",
    "    \n",
    "    # Remove rows with zero volume or non-positive close prices (often a sign of bad data)\n",
    "    initial_rows = df_clean.shape[0]\n",
    "    df_clean = df_clean[\n",
    "        (df_clean['Volume'] > 0) & \n",
    "        (df_clean['Close'] > 0)\n",
    "    ]\n",
    "    rows_removed = initial_rows - df_clean.shape[0]\n",
    "    print(f\"Removed {rows_removed} low-quality rows (Volume=0 or Close<=0).\")\n",
    "    \n",
    "    # 5. Final Output\n",
    "    \n",
    "    final_ticker_count = df_clean['Ticker'].nunique()\n",
    "    print(f\"\\n--- Preprocessing Summary ---\")\n",
    "    print(f\"Final dataset shape: {df_clean.shape}\")\n",
    "    print(f\"Final number of unique, active tickers: {final_ticker_count}\")\n",
    "    print(f\"Cleaned data saved to {OUTPUT_FILE_PATH}\")\n",
    "    \n",
    "    # Save the cleaned data to a new CSV file\n",
    "    df_clean.to_csv(OUTPUT_FILE_PATH, index=False)\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "# Run the preprocessing script\n",
    "if __name__ == \"__main__\":\n",
    "    preprocessed_data = load_and_preprocess_data(FILE_PATH)\n",
    "    \n",
    "    # Optional: Display a snapshot of the cleaned data\n",
    "    if preprocessed_data is not None:\n",
    "        print(\"\\n--- Snapshot of Preprocessed Data ---\")\n",
    "        print(preprocessed_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5ea7ac-d39e-457e-b08c-bcfb8d707112",
   "metadata": {},
   "source": [
    "#  Dataset 3: Alpha Vantage Stock Market Data API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55bcfad6-c820-4892-81a3-762c9e20e90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "📊 Alpha Vantage Multi-Key Stock Data Fetcher\n",
      "============================================================\n",
      "✅ 5 tickers with 5 dedicated API keys\n",
      "✅ Each ticker uses its own API key\n",
      "\n",
      "\n",
      "============================================================\n",
      "[1/5] Processing: AAPL\n",
      "============================================================\n",
      "   🔑 Using API key: 9VNG...YZMJ\n",
      "   📋 Response keys: ['Meta Data', 'Time Series (Daily)']\n",
      "   ✅ Successfully received data!\n",
      "   📈 Calculating SMA (20)...\n",
      "   📈 Calculating RSI (14)...\n",
      "   📈 Calculating MACD...\n",
      "   ✅ Successfully processed AAPL (6,554 rows)\n",
      "\n",
      "============================================================\n",
      "[2/5] Processing: MSFT\n",
      "============================================================\n",
      "⏳ Waiting 12 seconds...\n",
      "   🔑 Using API key: UVAA...7AGC\n",
      "   📋 Response keys: ['Meta Data', 'Time Series (Daily)']\n",
      "   ✅ Successfully received data!\n",
      "   📈 Calculating SMA (20)...\n",
      "   📈 Calculating RSI (14)...\n",
      "   📈 Calculating MACD...\n",
      "   ✅ Successfully processed MSFT (6,554 rows)\n",
      "\n",
      "============================================================\n",
      "[3/5] Processing: GOOG\n",
      "============================================================\n",
      "⏳ Waiting 12 seconds...\n",
      "   🔑 Using API key: 63BQ...PB9X\n",
      "   📋 Response keys: ['Meta Data', 'Time Series (Daily)']\n",
      "   ✅ Successfully received data!\n",
      "   📈 Calculating SMA (20)...\n",
      "   📈 Calculating RSI (14)...\n",
      "   📈 Calculating MACD...\n",
      "   ✅ Successfully processed GOOG (2,932 rows)\n",
      "\n",
      "============================================================\n",
      "[4/5] Processing: SPY\n",
      "============================================================\n",
      "⏳ Waiting 12 seconds...\n",
      "   🔑 Using API key: A574...PI7H\n",
      "   📋 Response keys: ['Meta Data', 'Time Series (Daily)']\n",
      "   ✅ Successfully received data!\n",
      "   📈 Calculating SMA (20)...\n",
      "   📈 Calculating RSI (14)...\n",
      "   📈 Calculating MACD...\n",
      "   ✅ Successfully processed SPY (6,554 rows)\n",
      "\n",
      "============================================================\n",
      "[5/5] Processing: QQQ\n",
      "============================================================\n",
      "⏳ Waiting 12 seconds...\n",
      "   🔑 Using API key: KHN5...IC21\n",
      "   📋 Response keys: ['Meta Data', 'Time Series (Daily)']\n",
      "   ✅ Successfully received data!\n",
      "   📈 Calculating SMA (20)...\n",
      "   📈 Calculating RSI (14)...\n",
      "   📈 Calculating MACD...\n",
      "   ✅ Successfully processed QQQ (6,554 rows)\n",
      "\n",
      "============================================================\n",
      "💾 Saving Results...\n",
      "============================================================\n",
      "\n",
      "🎉 SUCCESS!\n",
      "============================================================\n",
      "✅ Saved: alpha_vantage_dataset.csv\n",
      "✅ Total rows: 29,148\n",
      "✅ Tickers processed: AAPL, MSFT, GOOG, SPY, QQQ\n",
      "✅ Date range: 1999-11-01 to 2025-11-19\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------------------\n",
    "# CONFIGURATION\n",
    "# ---------------------\n",
    "# Each ticker gets its own API key\n",
    "TICKER_API_KEYS = {\n",
    "    \"AAPL\": \"9VNGCY5YXNT0YZMJ\",\n",
    "    \"MSFT\": \"UVAAAVPKVEHB7AGC\",\n",
    "    \"GOOG\": \"63BQ32DSX4HMPB9X\",\n",
    "    \"SPY\": \"A57406I9BSFDPI7H\",\n",
    "    \"QQQ\": \"KHN50642ENXFIC21\"\n",
    "}\n",
    "\n",
    "BASE_URL = \"https://www.alphavantage.co/query\"\n",
    "SECONDS_PER_CALL = 12  # Rate limiting between calls\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# Technical Indicator Functions\n",
    "# ===========================\n",
    "\n",
    "def calculate_sma(series, period=20):\n",
    "    \"\"\"Calculate Simple Moving Average\"\"\"\n",
    "    return series.rolling(window=period).mean()\n",
    "\n",
    "\n",
    "def calculate_rsi(series, period=14):\n",
    "    \"\"\"Calculate Relative Strength Index\"\"\"\n",
    "    delta = series.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
    "    \n",
    "    rs = gain / loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "\n",
    "def calculate_macd(series, fast=12, slow=26, signal=9):\n",
    "    \"\"\"Calculate MACD, Signal, and Histogram\"\"\"\n",
    "    exp1 = series.ewm(span=fast, adjust=False).mean()\n",
    "    exp2 = series.ewm(span=slow, adjust=False).mean()\n",
    "    \n",
    "    macd = exp1 - exp2\n",
    "    signal_line = macd.ewm(span=signal, adjust=False).mean()\n",
    "    histogram = macd - signal_line\n",
    "    \n",
    "    return macd, signal_line, histogram\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# API Call Function\n",
    "# ===========================\n",
    "\n",
    "def call_api(params, api_key, ticker):\n",
    "    \"\"\"Make API request with specific API key\"\"\"\n",
    "    params[\"apikey\"] = api_key\n",
    "    \n",
    "    key_display = f\"{api_key[:4]}...{api_key[-4:]}\"\n",
    "    print(f\"   🔑 Using API key: {key_display}\")\n",
    "    \n",
    "    max_retries = 3\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(BASE_URL, params=params, timeout=30)\n",
    "\n",
    "            if not response.text.strip():\n",
    "                print(f\"   ❗ Empty response (attempt {attempt + 1}/{max_retries})\")\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "\n",
    "            data = response.json()\n",
    "\n",
    "            print(f\"   📋 Response keys: {list(data.keys())}\")\n",
    "\n",
    "            if \"Note\" in data:\n",
    "                print(f\"   ❗ Rate limit hit: {data['Note']}\")\n",
    "                return None\n",
    "                \n",
    "            if \"Information\" in data:\n",
    "                print(f\"   ❗ API message: {data['Information']}\")\n",
    "                return None\n",
    "\n",
    "            if \"Error Message\" in data:\n",
    "                print(f\"   ❗ API Error: {data['Error Message']}\")\n",
    "                return None\n",
    "\n",
    "            if \"Time Series (Daily)\" in data:\n",
    "                print(f\"   ✅ Successfully received data!\")\n",
    "            else:\n",
    "                print(f\"   ⚠️ Unexpected response format\")\n",
    "                print(f\"   📄 Response preview: {str(data)[:300]}\")\n",
    "\n",
    "            return data\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   ❗ Error: {e} (attempt {attempt + 1}/{max_retries})\")\n",
    "            time.sleep(5)\n",
    "\n",
    "    print(\"   ❌ Failed after all retries\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# MAIN DOWNLOAD LOOP\n",
    "# ===========================\n",
    "\n",
    "all_data = []\n",
    "successful_tickers = []\n",
    "failed_tickers = []\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"📊 Alpha Vantage Multi-Key Stock Data Fetcher\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"✅ {len(TICKER_API_KEYS)} tickers with {len(TICKER_API_KEYS)} dedicated API keys\")\n",
    "print(f\"✅ Each ticker uses its own API key\\n\")\n",
    "\n",
    "for i, (ticker, api_key) in enumerate(TICKER_API_KEYS.items(), 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"[{i}/{len(TICKER_API_KEYS)}] Processing: {ticker}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    if i > 1:\n",
    "        print(f\"⏳ Waiting {SECONDS_PER_CALL} seconds...\")\n",
    "        time.sleep(SECONDS_PER_CALL)\n",
    "\n",
    "    # *** FREE ENDPOINT ***\n",
    "    params_daily = {\n",
    "        \"function\": \"TIME_SERIES_DAILY\",\n",
    "        \"symbol\": ticker,\n",
    "        \"outputsize\": \"full\"\n",
    "    }\n",
    "\n",
    "    data_daily = call_api(params_daily, api_key, ticker)\n",
    "\n",
    "    if data_daily is None or \"Time Series (Daily)\" not in data_daily:\n",
    "        print(f\"   ❌ Failed to fetch data for {ticker}\")\n",
    "        failed_tickers.append(ticker)\n",
    "        continue\n",
    "\n",
    "    df = pd.DataFrame.from_dict(data_daily[\"Time Series (Daily)\"], orient=\"index\")\n",
    "    df = df.rename_axis(\"Date\").reset_index()\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "    df[\"Ticker\"] = ticker\n",
    "\n",
    "    df = df.rename(columns={\n",
    "        \"1. open\": \"Open\",\n",
    "        \"2. high\": \"High\",\n",
    "        \"3. low\": \"Low\",\n",
    "        \"4. close\": \"Close\",\n",
    "        \"5. volume\": \"Volume\"\n",
    "    })\n",
    "\n",
    "    numeric_cols = [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]\n",
    "    for col in numeric_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    df = df.sort_values(\"Date\").reset_index(drop=True)\n",
    "\n",
    "    print(\"   📈 Calculating SMA (20)...\")\n",
    "    df[\"SMA_20\"] = calculate_sma(df[\"Close\"], period=20)\n",
    "\n",
    "    print(\"   📈 Calculating RSI (14)...\")\n",
    "    df[\"RSI_14\"] = calculate_rsi(df[\"Close\"], period=14)\n",
    "\n",
    "    print(\"   📈 Calculating MACD...\")\n",
    "    df[\"MACD\"], df[\"MACD_Signal\"], df[\"MACD_Hist\"] = calculate_macd(df[\"Close\"])\n",
    "\n",
    "    all_data.append(df)\n",
    "    successful_tickers.append(ticker)\n",
    "    print(f\"   ✅ Successfully processed {ticker} ({len(df):,} rows)\")\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# SAVE FINAL DATASET\n",
    "# ===========================\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"💾 Saving Results...\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if all_data:\n",
    "    final_df = pd.concat(all_data, ignore_index=True)\n",
    "    final_df = final_df.sort_values([\"Ticker\", \"Date\"])\n",
    "\n",
    "    column_order = [\n",
    "        \"Ticker\", \"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\",\n",
    "        \"SMA_20\", \"RSI_14\", \"MACD\", \"MACD_Signal\", \"MACD_Hist\"\n",
    "    ]\n",
    "    final_df = final_df[column_order]\n",
    "\n",
    "    output_file = \"alpha_vantage_dataset.csv\"\n",
    "    final_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"\\n🎉 SUCCESS!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"✅ Saved: {output_file}\")\n",
    "    print(f\"✅ Total rows: {len(final_df):,}\")\n",
    "    print(f\"✅ Tickers processed: {', '.join(successful_tickers)}\")\n",
    "    print(f\"✅ Date range: {final_df['Date'].min().date()} to {final_df['Date'].max().date()}\")\n",
    "    \n",
    "    if failed_tickers:\n",
    "        print(f\"\\n⚠️ Failed tickers: {', '.join(failed_tickers)}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n❌ No data was successfully fetched.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b295446-4003-4cf7-8daf-08883eb2965f",
   "metadata": {},
   "source": [
    "#  Dataset 4: Fama-French Factor Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2967fbf-171d-4958-8c95-ef8234e146f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'str' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:218\u001b[0m, in \u001b[0;36m_na_arithmetic_op\u001b[1;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 218\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(left, right)\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\computation\\expressions.py:242\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(op, a, b, use_numexpr)\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_numexpr:\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;66;03m# error: \"None\" not callable\u001b[39;00m\n\u001b[1;32m--> 242\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _evaluate(op, op_str, a, b)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _evaluate_standard(op, op_str, a, b)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\computation\\expressions.py:131\u001b[0m, in \u001b[0;36m_evaluate_numexpr\u001b[1;34m(op, op_str, a, b)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 131\u001b[0m     result \u001b[38;5;241m=\u001b[39m _evaluate_standard(op, op_str, a, b)\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\computation\\expressions.py:73\u001b[0m, in \u001b[0;36m_evaluate_standard\u001b[1;34m(op, op_str, a, b)\u001b[0m\n\u001b[0;32m     72\u001b[0m     _store_test_result(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op(a, b)\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'str' and 'float'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m df\u001b[38;5;241m.\u001b[39mset_index(df\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m0\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     15\u001b[0m df\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m), \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m pd\u001b[38;5;241m.\u001b[39moffsets\u001b[38;5;241m.\u001b[39mMonthEnd(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m100.0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:10374\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m  10360\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[0;32m  10362\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[0;32m  10363\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  10364\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10372\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m  10373\u001b[0m )\n\u001b[1;32m> 10374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mapply()\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[1;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_generator()\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[0;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(v, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[11], line 16\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     14\u001b[0m df\u001b[38;5;241m.\u001b[39mset_index(df\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m0\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     15\u001b[0m df\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m), \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m pd\u001b[38;5;241m.\u001b[39moffsets\u001b[38;5;241m.\u001b[39mMonthEnd(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m100.0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m method(\u001b[38;5;28mself\u001b[39m, other)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\arraylike.py:210\u001b[0m, in \u001b[0;36mOpsMixin.__truediv__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__truediv__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__truediv__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m--> 210\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_arith_method(other, operator\u001b[38;5;241m.\u001b[39mtruediv)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:6135\u001b[0m, in \u001b[0;36mSeries._arith_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   6133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_arith_method\u001b[39m(\u001b[38;5;28mself\u001b[39m, other, op):\n\u001b[0;32m   6134\u001b[0m     \u001b[38;5;28mself\u001b[39m, other \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_align_for_op(other)\n\u001b[1;32m-> 6135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m base\u001b[38;5;241m.\u001b[39mIndexOpsMixin\u001b[38;5;241m.\u001b[39m_arith_method(\u001b[38;5;28mself\u001b[39m, other, op)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\base.py:1382\u001b[0m, in \u001b[0;36mIndexOpsMixin._arith_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   1379\u001b[0m     rvalues \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(rvalues\u001b[38;5;241m.\u001b[39mstart, rvalues\u001b[38;5;241m.\u001b[39mstop, rvalues\u001b[38;5;241m.\u001b[39mstep)\n\u001b[0;32m   1381\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1382\u001b[0m     result \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39marithmetic_op(lvalues, rvalues, op)\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(result, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:283\u001b[0m, in \u001b[0;36marithmetic_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    279\u001b[0m     _bool_arith_check(op, left, right)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;66;03m# error: Argument 1 to \"_na_arithmetic_op\" has incompatible type\u001b[39;00m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;66;03m# \"Union[ExtensionArray, ndarray[Any, Any]]\"; expected \"ndarray[Any, Any]\"\u001b[39;00m\n\u001b[1;32m--> 283\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m _na_arithmetic_op(left, right, op)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res_values\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:227\u001b[0m, in \u001b[0;36m_na_arithmetic_op\u001b[1;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cmp \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m    221\u001b[0m         left\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(right, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m\n\u001b[0;32m    222\u001b[0m     ):\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;66;03m# Don't do this for comparisons, as that will handle complex numbers\u001b[39;00m\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;66;03m#  incorrectly, see GH#32047\u001b[39;00m\n\u001b[1;32m--> 227\u001b[0m         result \u001b[38;5;241m=\u001b[39m _masked_arith_op(left, right, op)\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:182\u001b[0m, in \u001b[0;36m_masked_arith_op\u001b[1;34m(x, y, op)\u001b[0m\n\u001b[0;32m    179\u001b[0m         mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(y \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m, mask)\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m--> 182\u001b[0m         result[mask] \u001b[38;5;241m=\u001b[39m op(xrav[mask], y)\n\u001b[0;32m    184\u001b[0m np\u001b[38;5;241m.\u001b[39mputmask(result, \u001b[38;5;241m~\u001b[39mmask, np\u001b[38;5;241m.\u001b[39mnan)\n\u001b[0;32m    185\u001b[0m result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# 2D compat\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'str' and 'float'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "url = \"https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/F-F_Research_Data_Factors_CSV.zip\"\n",
    "urllib.request.urlretrieve(url, \"ff3.zip\")\n",
    "\n",
    "with zipfile.ZipFile(\"ff3.zip\", \"r\") as z:\n",
    "    z.extractall()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8af4aa75-0017-4a48-94fa-90fe3d70c64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# Load your dataset\n",
    "# ----------------------------\n",
    "df = pd.read_csv(\"F-F_Research_Data_Factors.csv\")\n",
    "\n",
    "# ----------------------------\n",
    "# Convert YYYYMM → datetime\n",
    "# ----------------------------\n",
    "date_col = df.columns[0]      # First column is the date\n",
    "df[date_col] = pd.to_datetime(df[date_col].astype(str), format=\"%Y%m\")\n",
    "\n",
    "# ----------------------------\n",
    "# Fill NaN values\n",
    "# ----------------------------\n",
    "df = df.ffill().bfill()       # Forward fill → backward fill\n",
    "\n",
    "# ----------------------------\n",
    "# Show results\n",
    "# ----------------------------\n",
    "df\n",
    "df.to_csv('F-F_Research_Data_Factors.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8f46e4-fa1a-4b5f-91c1-036be4da1703",
   "metadata": {},
   "source": [
    "# Dataset 5: CoinGecko Cryptocurrency Data API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b738c253-0cd1-4a6d-ae10-b7d85f187b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching bitcoin in usd...\n",
      "Fetching bitcoin in eur...\n",
      "Fetching ethereum in usd...\n",
      "Fetching ethereum in eur...\n",
      "                       usd_price  usd_market_cap  usd_volume_24h  \\\n",
      "coin    timestamp                                                  \n",
      "bitcoin 2024-11-21  94217.022296    1.862619e+12    8.074727e+10   \n",
      "        2024-11-22  98509.118591    1.948285e+12    1.181639e+11   \n",
      "        2024-11-23  98927.494946    1.958091e+12    8.574617e+10   \n",
      "        2024-11-24  97679.463816    1.931256e+12    4.741420e+10   \n",
      "        2024-11-25  98015.935529    1.939446e+12    5.066568e+10   \n",
      "\n",
      "                       eur_price  eur_market_cap  eur_volume_24h  \n",
      "coin    timestamp                                                 \n",
      "bitcoin 2024-11-21  89326.405103    1.766102e+12    7.655584e+10  \n",
      "        2024-11-22  94069.115598    1.861206e+12    1.128380e+11  \n",
      "        2024-11-23  94955.556023    1.879474e+12    8.230347e+10  \n",
      "        2024-11-24  93760.270690    1.853768e+12    4.551180e+10  \n",
      "        2024-11-25  93546.114821    1.850862e+12    4.835517e+10  \n",
      "Saved crypto_365days_concatenated.csv\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# CoinGecko Crypto Data Loader + Concatenate All Coins\n",
    "# Fetches past 365 days of historical price, market cap, and 24h volume\n",
    "# Handles multiple coins and fiat currencies\n",
    "# Concatenates all coins into a single multi-index DataFrame\n",
    "# ==========================================\n",
    "\n",
    "import pandas as pd\n",
    "from pycoingecko import CoinGeckoAPI\n",
    "import time\n",
    "\n",
    "class CryptoDataLoader:\n",
    "    def __init__(self, coins=None, vs_currencies=['usd'], top_n=10):\n",
    "        \"\"\"\n",
    "        coins: list of coin IDs, e.g., ['bitcoin','ethereum']\n",
    "        vs_currencies: list of fiat currencies, e.g., ['usd','eur']\n",
    "        top_n: number of top coins by market cap if coins is None\n",
    "        \"\"\"\n",
    "        self.cg = CoinGeckoAPI()\n",
    "        self.coins = coins\n",
    "        self.vs_currencies = vs_currencies\n",
    "        self.top_n = top_n\n",
    "\n",
    "    def get_top_coins(self):\n",
    "        \"\"\"Get top N coins by market cap.\"\"\"\n",
    "        markets = self.cg.get_coins_markets(vs_currency='usd', per_page=self.top_n, page=1)\n",
    "        return [m['id'] for m in markets]\n",
    "\n",
    "    def fetch_historical(self, coin_id, vs_currency='usd', days=365):\n",
    "        \"\"\"Fetch past N days of historical data for one coin in one currency.\"\"\"\n",
    "        data = self.cg.get_coin_market_chart_by_id(\n",
    "            id=coin_id,\n",
    "            vs_currency=vs_currency,\n",
    "            days=days  # free tier limit\n",
    "        )\n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(data['prices'], columns=['timestamp', 'price'])\n",
    "        df['market_cap'] = [x[1] for x in data['market_caps']]\n",
    "        df['volume_24h'] = [x[1] for x in data['total_volumes']]\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "        df.set_index('timestamp', inplace=True)\n",
    "        return df\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"Fetch historical data for all coins and currencies.\"\"\"\n",
    "        if self.coins is None:\n",
    "            self.coins = self.get_top_coins()\n",
    "        all_data = []\n",
    "        for coin in self.coins:\n",
    "            coin_frames = []\n",
    "            for currency in self.vs_currencies:\n",
    "                print(f\"Fetching {coin} in {currency}...\")\n",
    "                df = self.fetch_historical(coin, currency)\n",
    "                # Prefix columns with currency\n",
    "                df = df.add_prefix(f\"{currency}_\")\n",
    "                coin_frames.append(df)\n",
    "                time.sleep(1)  # avoid hitting rate limits\n",
    "            # Combine multiple currencies horizontally\n",
    "            combined = pd.concat(coin_frames, axis=1)\n",
    "            # Add coin ID as a column for concatenation\n",
    "            combined['coin'] = coin\n",
    "            all_data.append(combined)\n",
    "        # Concatenate all coins into a single DataFrame\n",
    "        final_df = pd.concat(all_data)\n",
    "        # Set multi-index: coin -> timestamp\n",
    "        final_df.set_index('coin', append=True, inplace=True)\n",
    "        final_df = final_df.reorder_levels(['coin', final_df.index.names[0]])\n",
    "        final_df.sort_index(inplace=True)\n",
    "        return final_df\n",
    "\n",
    "# ==========================================\n",
    "# Example usage\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    loader = CryptoDataLoader(coins=['bitcoin', 'ethereum'], vs_currencies=['usd', 'eur'])\n",
    "    df_all = loader.load()\n",
    "\n",
    "    # Show top rows\n",
    "    print(df_all.head())\n",
    "\n",
    "    # Save concatenated DataFrame to CSV\n",
    "    df_all.to_csv(\"crypto_365days_concatenated.csv\")\n",
    "    print(\"Saved crypto_365days_concatenated.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc4b36a-71db-4f20-8692-074276f20b26",
   "metadata": {},
   "source": [
    "# Dataset 6: ETF & Mutual Fund Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be52bdc1-6a53-48e8-9359-f5c799a81522",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\itsam\\AppData\\Local\\Temp\\ipykernel_13168\\3958645185.py:26: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_filled = df.fillna(method='ffill')\n",
      "C:\\Users\\itsam\\AppData\\Local\\Temp\\ipykernel_13168\\3958645185.py:29: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_filled = df_filled.fillna(method='bfill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of cleaned data:\n",
      "            Algerian dinar   (DZD)  Australian dollar   (AUD)  \\\n",
      "Date                                                            \n",
      "1994-01-03                 74.9996                     0.6873   \n",
      "1994-01-04                 74.9996                     0.6873   \n",
      "1994-01-05                 74.9996                     0.6870   \n",
      "1994-01-06                 74.9996                     0.6857   \n",
      "1994-01-07                 74.9996                     0.6857   \n",
      "\n",
      "            Austrian schilling   (ATS)  Belgian franc   (BEF)  \\\n",
      "Date                                                            \n",
      "1994-01-03                      12.194                 32.025   \n",
      "1994-01-04                      12.234                 32.025   \n",
      "1994-01-05                      12.187                 32.025   \n",
      "1994-01-06                      12.187                 32.025   \n",
      "1994-01-07                      12.235                 32.025   \n",
      "\n",
      "            Botswana pula   (BWP)  Brazilian real   (BRL)  \\\n",
      "Date                                                        \n",
      "1994-01-03                4.73821                 326.095   \n",
      "1994-01-04                4.73821                 331.210   \n",
      "1994-01-05                4.73821                 336.470   \n",
      "1994-01-06                4.73821                 341.810   \n",
      "1994-01-07                4.73821                 347.225   \n",
      "\n",
      "            Brunei dollar   (BND)  Canadian dollar   (CAD)  \\\n",
      "Date                                                         \n",
      "1994-01-03                  1.736                   1.3174   \n",
      "1994-01-04                  1.736                   1.3174   \n",
      "1994-01-05                  1.736                   1.3180   \n",
      "1994-01-06                  1.736                   1.3211   \n",
      "1994-01-07                  1.736                   1.3234   \n",
      "\n",
      "            Chilean peso   (CLP)  Chinese yuan   (CNY)  ...  \\\n",
      "Date                                                    ...   \n",
      "1994-01-03                417.73                   8.7  ...   \n",
      "1994-01-04                417.73                   8.7  ...   \n",
      "1994-01-05                417.73                   8.7  ...   \n",
      "1994-01-06                417.73                   8.7  ...   \n",
      "1994-01-07                417.73                   8.7  ...   \n",
      "\n",
      "            South African rand   (ZAR)  Spanish peseta   (ESP)  \\\n",
      "Date                                                             \n",
      "1994-01-03                         3.4                  142.81   \n",
      "1994-01-04                         3.4                  143.57   \n",
      "1994-01-05                         3.4                  143.74   \n",
      "1994-01-06                         3.4                  143.74   \n",
      "1994-01-07                         3.4                  145.61   \n",
      "\n",
      "            Swedish krona   (SEK)  Swiss franc   (CHF)  Thai baht   (THB)  \\\n",
      "Date                                                                        \n",
      "1994-01-03                 8.3630               1.4790              25.58   \n",
      "1994-01-04                 8.3685               1.4870              25.58   \n",
      "1994-01-05                 8.2435               1.4790              25.56   \n",
      "1994-01-06                 8.2435               1.4750              25.57   \n",
      "1994-01-07                 8.2090               1.4805              25.55   \n",
      "\n",
      "            Trinidadian dollar   (TTD)  U.A.E. dirham   (AED)  \\\n",
      "Date                                                            \n",
      "1994-01-03                     5.83400                  3.671   \n",
      "1994-01-04                     5.82041                  3.671   \n",
      "1994-01-05                     5.82987                  3.671   \n",
      "1994-01-06                     5.82018                  3.671   \n",
      "1994-01-07                     5.80606                  3.671   \n",
      "\n",
      "            U.K. pound   (GBP)  U.S. dollar   (USD)  Uruguayan peso   (UYU)  \n",
      "Date                                                                         \n",
      "1994-01-03              1.4820                  1.0                    9.32  \n",
      "1994-01-04              1.4820                  1.0                    9.32  \n",
      "1994-01-05              1.4862                  1.0                    9.32  \n",
      "1994-01-06              1.4843                  1.0                    9.32  \n",
      "1994-01-07              1.4836                  1.0                    9.32  \n",
      "\n",
      "[5 rows x 53 columns]\n",
      "\n",
      "Number of remaining NaNs (should be 0):\n",
      "6737\n",
      "\n",
      "Cleaned CSV saved as 'Exchange_Rate_Report_Cleaned.csv'\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Exchange Rate Data Preprocessing Script\n",
    "# Loads CSV, cleans headers, converts to numeric, fills NaNs, and saves cleaned CSV\n",
    "# ==========================================\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# --------------------------\n",
    "# Step 1: Load CSV\n",
    "# --------------------------\n",
    "df = pd.read_csv(\"Exchange_Rate_Report.csv\", index_col=0)\n",
    "\n",
    "# Strip extra spaces from column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Convert all columns to numeric (non-numeric -> NaN)\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Convert index to datetime\n",
    "df.index = pd.to_datetime(df.index, errors='coerce')\n",
    "\n",
    "# --------------------------\n",
    "# Step 2: Fill NaN values\n",
    "# --------------------------\n",
    "# 1. Forward fill missing values\n",
    "df_filled = df.fillna(method='ffill')\n",
    "\n",
    "# 2. Backward fill remaining missing values\n",
    "df_filled = df_filled.fillna(method='bfill')\n",
    "\n",
    "# 3. Linear interpolation for any remaining small gaps\n",
    "df_filled = df_filled.interpolate(method='linear')\n",
    "\n",
    "# --------------------------\n",
    "# Step 3: Verify\n",
    "# --------------------------\n",
    "print(\"First 5 rows of cleaned data:\")\n",
    "print(df_filled.head())\n",
    "\n",
    "print(\"\\nNumber of remaining NaNs (should be 0):\")\n",
    "print(df_filled.isna().sum().sum())\n",
    "\n",
    "# --------------------------\n",
    "# Step 4: Save cleaned CSV\n",
    "# --------------------------\n",
    "df_filled.to_csv(\"Exchange_Rate_Report_Cleaned.csv\")\n",
    "print(\"\\nCleaned CSV saved as 'Exchange_Rate_Report_Cleaned.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e90d8f-9b93-4b24-aa1b-e010e9eec0a9",
   "metadata": {},
   "source": [
    "# Dataset 7: FRED Economic Indicators (for portfolio context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "952cef96-892a-4a78-ab88-ff92af393b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching DGS3MO (3mo_treasury_yield)...\n",
      "Fetching DGS2 (2yr_treasury_yield)...\n",
      "Fetching DGS10 (10yr_treasury_yield)...\n",
      "Fetching DGS30 (30yr_treasury_yield)...\n",
      "Fetching UNRATE (unemployment_rate)...\n",
      "Fetching PCEPI (pce_inflation)...\n",
      "Fetching VIXCLS (vix)...\n",
      "First 5 rows of combined FRED indicators:\n",
      "            3mo_treasury_yield  2yr_treasury_yield  10yr_treasury_yield  \\\n",
      "1948-01-01               17.01                7.26                 4.06   \n",
      "1948-02-01               17.01                7.26                 4.06   \n",
      "1948-03-01               17.01                7.26                 4.06   \n",
      "1948-04-01               17.01                7.26                 4.06   \n",
      "1948-05-01               17.01                7.26                 4.06   \n",
      "\n",
      "            30yr_treasury_yield  unemployment_rate  pce_inflation    vix  \n",
      "1948-01-01                  7.7                3.4         15.164  17.24  \n",
      "1948-02-01                  7.7                3.8         15.164  17.24  \n",
      "1948-03-01                  7.7                4.0         15.164  17.24  \n",
      "1948-04-01                  7.7                3.9         15.164  17.24  \n",
      "1948-05-01                  7.7                3.5         15.164  17.24  \n",
      "\n",
      "Number of remaining NaNs (should be 0): 0\n",
      "\n",
      "Saved combined FRED data as 'FRED_Economic_Indicators.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\itsam\\AppData\\Local\\Temp\\ipykernel_13168\\1073717842.py:47: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_all = df_all.fillna(method='ffill')\n",
      "C:\\Users\\itsam\\AppData\\Local\\Temp\\ipykernel_13168\\1073717842.py:48: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_all = df_all.fillna(method='bfill')\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# FRED Economic Indicators Loader\n",
    "# ==========================================\n",
    "\n",
    "import pandas as pd\n",
    "from fredapi import Fred\n",
    "\n",
    "# --------------------------\n",
    "# Step 1: Initialize FRED API\n",
    "# --------------------------\n",
    "api_key = '7242fe69a51188cad3114480ea35cf15'\n",
    "fred = Fred(api_key=api_key)\n",
    "\n",
    "# --------------------------\n",
    "# Step 2: Define FRED series IDs\n",
    "# --------------------------\n",
    "series_ids = {\n",
    "    \"3mo_treasury_yield\": \"DGS3MO\",\n",
    "    \"2yr_treasury_yield\": \"DGS2\",\n",
    "    \"10yr_treasury_yield\": \"DGS10\",\n",
    "    \"30yr_treasury_yield\": \"DGS30\",\n",
    "    \"unemployment_rate\": \"UNRATE\",\n",
    "    \"pce_inflation\": \"PCEPI\",\n",
    "    \"vix\": \"VIXCLS\"\n",
    "}\n",
    "\n",
    "# --------------------------\n",
    "# Step 3: Fetch data\n",
    "# --------------------------\n",
    "data_frames = []\n",
    "\n",
    "for name, series_id in series_ids.items():\n",
    "    print(f\"Fetching {series_id} ({name})...\")\n",
    "    df = fred.get_series(series_id)\n",
    "    df = df.to_frame(name)   # convert to DataFrame with column name\n",
    "    data_frames.append(df)\n",
    "\n",
    "# --------------------------\n",
    "# Step 4: Combine all series into one DataFrame\n",
    "# --------------------------\n",
    "df_all = pd.concat(data_frames, axis=1)\n",
    "\n",
    "# --------------------------\n",
    "# Step 5: Handle missing data\n",
    "# --------------------------\n",
    "# Forward fill, backward fill, interpolate if needed\n",
    "df_all = df_all.fillna(method='ffill')\n",
    "df_all = df_all.fillna(method='bfill')\n",
    "df_all = df_all.interpolate(method='linear')\n",
    "\n",
    "# --------------------------\n",
    "# Step 6: Verify\n",
    "# --------------------------\n",
    "print(\"First 5 rows of combined FRED indicators:\")\n",
    "print(df_all.head())\n",
    "\n",
    "print(\"\\nNumber of remaining NaNs (should be 0):\", df_all.isna().sum().sum())\n",
    "\n",
    "# --------------------------\n",
    "# Step 7: Save to CSV\n",
    "# --------------------------\n",
    "df_all.to_csv(\"FRED_Economic_Indicators.csv\")\n",
    "print(\"\\nSaved combined FRED data as 'FRED_Economic_Indicators.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69895ec4-2911-4118-92a8-fdfc29b5f791",
   "metadata": {},
   "source": [
    "# 3. TAX OPTIMIZATION AGENT DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc0b0697-fa17-4aaf-abf2-3d3f3586c9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 1040 from https://www.irs.gov/pub/irs-pdf/f1040.pdf...\n",
      "Saved f1040.pdf\n",
      "Skipping 1040A (no mapping found)\n",
      "Skipping 1040EZ (no mapping found)\n",
      "Skipping 1040SR (no mapping found)\n",
      "Downloading Schedule A from https://www.irs.gov/pub/irs-pdf/f1040sa.pdf...\n",
      "Saved f1040sa.pdf\n",
      "Downloading Schedule B from https://www.irs.gov/pub/irs-pdf/f1040sb.pdf...\n",
      "Saved f1040sb.pdf\n",
      "Downloading Schedule C from https://www.irs.gov/pub/irs-pdf/f1040sc.pdf...\n",
      "Saved f1040sc.pdf\n",
      "Downloading Schedule D from https://www.irs.gov/pub/irs-pdf/f1040sd.pdf...\n",
      "Saved f1040sd.pdf\n",
      "Downloading Schedule E from https://www.irs.gov/pub/irs-pdf/f1040se.pdf...\n",
      "Saved f1040se.pdf\n",
      "Downloading Schedule SE from https://www.irs.gov/pub/irs-pdf/f1040sese.pdf...\n",
      "Failed to download Schedule SE: Status code 404\n",
      "Skipping Form 1099 (no mapping found)\n",
      "Skipping Form W-2 (no mapping found)\n",
      "Downloading Form 4562 from https://www.irs.gov/pub/irs-pdf/f4562.pdf...\n",
      "Saved f4562.pdf\n",
      "Downloading Form 8829 from https://www.irs.gov/pub/irs-pdf/f8829.pdf...\n",
      "Saved f8829.pdf\n",
      "Downloading Form 8949 from https://www.irs.gov/pub/irs-pdf/f8949.pdf...\n",
      "Saved f8949.pdf\n",
      "\n",
      "All downloads completed!\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# IRS Tax Forms Downloader for Tax Optimization\n",
    "# Downloads all relevant IRS forms for analysis\n",
    "# ==========================================\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# --------------------------\n",
    "# Step 1: Setup directories\n",
    "# --------------------------\n",
    "year = 2024\n",
    "base_dir = f\"IRS_Forms_{year}\"\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "# --------------------------\n",
    "# Step 2: List of target forms\n",
    "# --------------------------\n",
    "# Core forms relevant for tax optimization\n",
    "forms_to_download = [\n",
    "    \"1040\",      # Individual income\n",
    "    \"1040A\",     # Simplified (if available)\n",
    "    \"1040EZ\",    # Simplified (if available)\n",
    "    \"1040SR\",    # Senior\n",
    "    \"Schedule A\", # Itemized deductions\n",
    "    \"Schedule B\", # Interest/dividends\n",
    "    \"Schedule C\", # Business profit/loss\n",
    "    \"Schedule D\", # Capital gains/losses\n",
    "    \"Schedule E\", # Rental income\n",
    "    \"Schedule SE\", # Self-employment tax\n",
    "    \"Form 1099\",  # Various 1099s\n",
    "    \"Form W-2\",   # Wage reporting\n",
    "    \"Form 4562\",  # Depreciation & amortization\n",
    "    \"Form 8829\",  # Home office deduction\n",
    "    \"Form 8949\",  # Sales & other dispositions of capital assets\n",
    "]\n",
    "\n",
    "# --------------------------\n",
    "# Step 3: IRS forms base URL\n",
    "# --------------------------\n",
    "irs_base_pdf_url = \"https://www.irs.gov/pub/irs-pdf/\"\n",
    "\n",
    "# Map form name to actual PDF file names (approximate; IRS naming conventions)\n",
    "form_pdf_map = {\n",
    "    \"1040\": f\"f1040.pdf\",\n",
    "    \"Schedule A\": f\"f1040sa.pdf\",\n",
    "    \"Schedule B\": f\"f1040sb.pdf\",\n",
    "    \"Schedule C\": f\"f1040sc.pdf\",\n",
    "    \"Schedule D\": f\"f1040sd.pdf\",\n",
    "    \"Schedule E\": f\"f1040se.pdf\",\n",
    "    \"Schedule SE\": f\"f1040sese.pdf\",\n",
    "    \"Form 4562\": f\"f4562.pdf\",\n",
    "    \"Form 8829\": f\"f8829.pdf\",\n",
    "    \"Form 8949\": f\"f8949.pdf\",\n",
    "    # Add more mappings as needed\n",
    "}\n",
    "\n",
    "# --------------------------\n",
    "# Step 4: Download forms\n",
    "# --------------------------\n",
    "for form in forms_to_download:\n",
    "    pdf_file = form_pdf_map.get(form)\n",
    "    if not pdf_file:\n",
    "        print(f\"Skipping {form} (no mapping found)\")\n",
    "        continue\n",
    "\n",
    "    url = irs_base_pdf_url + pdf_file\n",
    "    print(f\"Downloading {form} from {url}...\")\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            save_path = os.path.join(base_dir, pdf_file)\n",
    "            with open(save_path, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"Saved {pdf_file}\")\n",
    "        else:\n",
    "            print(f\"Failed to download {form}: Status code {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {form}: {e}\")\n",
    "\n",
    "print(\"\\nAll downloads completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e922c7bb-c17c-4a0a-b69b-0e29bbd2d353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching IRS Statistics main page...\n",
      "Found 39 subpages.\n",
      "\n",
      "📄 Checking page: https://www.irs.gov/statistics/soi-tax-stats-historical-data-tables\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/histab21c.xlsx\n",
      "Saved → histab21c.xlsx\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/histab21d.xlsx\n",
      "Saved → histab21d.xlsx\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/histab21e.xlsx\n",
      "Saved → histab21e.xlsx\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/histab21f.xlsx\n",
      "Saved → histab21f.xlsx\n",
      "\n",
      "📄 Checking page: https://www.irs.gov/statistics/credits-and-deductions-gap\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/24rpcreditdeductiongapsummary.pdf\n",
      "Saved → 24rpcreditdeductiongapsummary.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/24rpeitcunderclaims.pdf\n",
      "Saved → 24rpeitcunderclaims.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/24rpctcunderclaims.pdf\n",
      "Saved → 24rpctcunderclaims.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/24rpedtcunderclaims.pdf\n",
      "Saved → 24rpedtcunderclaims.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/24rpeitcunderclaimsty2021census.pdf\n",
      "Saved → 24rpeitcunderclaimsty2021census.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/24rpctcunderclaimsty2020census.pdf\n",
      "Saved → 24rpctcunderclaimsty2020census.pdf\n",
      "\n",
      "📄 Checking page: https://www.irs.gov/statistics/soi-tax-stats-irs-tax-exempt-organization-population-data\n",
      "\n",
      "📄 Checking page: https://www.irs.gov/statistics/soi-tax-stats-s-corporation-statistics\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/14cotablecrosswalkccr.pdf\n",
      "Saved → 14cotablecrosswalkccr.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/03scorp.pdf\n",
      "Saved → 03scorp.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/02scorp.pdf\n",
      "Saved → 02scorp.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/01scorp.pdf\n",
      "Saved → 01scorp.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/00scorp.pdf\n",
      "Saved → 00scorp.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/99scorp.pdf\n",
      "Saved → 99scorp.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/98scorp.pdf\n",
      "Saved → 98scorp.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/97scorp.pdf\n",
      "Saved → 97scorp.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/96scorp.pdf\n",
      "Saved → 96scorp.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/95scorp.pdf\n",
      "Saved → 95scorp.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/17co24ccr.xlsx\n",
      "Saved → 17co24ccr.xlsx\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/16co24ccr.xlsx\n",
      "Saved → 16co24ccr.xlsx\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/15co24ccr.xlsx\n",
      "Saved → 15co24ccr.xlsx\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/14co24ccr.xlsx\n",
      "Saved → 14co24ccr.xlsx\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/17co24accr.xlsx\n",
      "Saved → 17co24accr.xlsx\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/16co24accr.xlsx\n",
      "Saved → 16co24accr.xlsx\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/15co24accr.xlsx\n",
      "Saved → 15co24accr.xlsx\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/14co24accr.xlsx\n",
      "Saved → 14co24accr.xlsx\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/17co32ccr.xlsx\n",
      "Saved → 17co32ccr.xlsx\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/16co32ccr.xlsx\n",
      "Saved → 16co32ccr.xlsx\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/15co32ccr.xlsx\n",
      "Saved → 15co32ccr.xlsx\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/14co32ccr.xlsx\n",
      "Saved → 14co32ccr.xlsx\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/17co61ccr.xlsx\n",
      "Saved → 17co61ccr.xlsx\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/16co61ccr.xlsx\n",
      "Saved → 16co61ccr.xlsx\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/15co61ccr.xlsx\n",
      "Saved → 15co61ccr.xlsx\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/14co61ccr.xlsx\n",
      "Saved → 14co61ccr.xlsx\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/17co62ccr.xlsx\n",
      "Saved → 17co62ccr.xlsx\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/16co62ccr.xlsx\n",
      "Saved → 16co62ccr.xlsx\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/15co62ccr.xlsx\n",
      "Saved → 15co62ccr.xlsx\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/14co62ccr.xlsx\n",
      "Saved → 14co62ccr.xlsx\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/17co07ccr.xlsx\n",
      "Saved → 17co07ccr.xlsx\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/16co07ccr.xlsx\n",
      "Saved → 16co07ccr.xlsx\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/15co07ccr.xlsx\n",
      "Saved → 15co07ccr.xlsx\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/14co07ccr.xlsx\n",
      "Saved → 14co07ccr.xlsx\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/17co08ccr.xlsx\n",
      "Saved → 17co08ccr.xlsx\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/16co08ccr.xlsx\n",
      "Saved → 16co08ccr.xlsx\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/15co08ccr.xlsx\n",
      "Saved → 15co08ccr.xlsx\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/14co08ccr.xlsx\n",
      "Saved → 14co08ccr.xlsx\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/17co09ccr.xlsx\n",
      "Saved → 17co09ccr.xlsx\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/16co09ccr.xlsx\n",
      "Saved → 16co09ccr.xlsx\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/15co09ccr.xlsx\n",
      "Saved → 15co09ccr.xlsx\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/14co09ccr.xlsx\n",
      "Saved → 14co09ccr.xlsx\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/07cofigas.xls\n",
      "Saved → 07cofigas.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/06cofigas.xls\n",
      "Saved → 06cofigas.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/05co1120sfiga.xls\n",
      "Saved → 05co1120sfiga.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/04co1120sfiga.xls\n",
      "Saved → 04co1120sfiga.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/07cofigbs.pdf\n",
      "Saved → 07cofigbs.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/06cofigbs.pdf\n",
      "Saved → 06cofigbs.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/05co1120sfigb.pdf\n",
      "Saved → 05co1120sfigb.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/04co1120sfigb.xls\n",
      "Saved → 04co1120sfigb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/06cofigcs.xls\n",
      "Saved → 06cofigcs.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/05co1120sfigc.xls\n",
      "Saved → 05co1120sfigc.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/04co1120sfigc.xls\n",
      "Saved → 04co1120sfigc.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/07cofigds.xls\n",
      "Saved → 07cofigds.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/06cofigds.xls\n",
      "Saved → 06cofigds.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/05co1120sfigd.xls\n",
      "Saved → 05co1120sfigd.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/04co1120sfigd.xls\n",
      "Saved → 04co1120sfigd.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/07cofiges.pdf\n",
      "Saved → 07cofiges.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/06cofiges.pdf\n",
      "Saved → 06cofiges.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/05co1120sfige.pdf\n",
      "Saved → 05co1120sfige.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/04co1120sfige.xls\n",
      "Saved → 04co1120sfige.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/07cofigfs.pdf\n",
      "Saved → 07cofigfs.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/06cofigfs.pdf\n",
      "Saved → 06cofigfs.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/05co1120sfigf.pdf\n",
      "Saved → 05co1120sfigf.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/04co1120sfigf.xls\n",
      "Saved → 04co1120sfigf.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/07cofiggs.xls\n",
      "Saved → 07cofiggs.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/06cofiggs.xls\n",
      "Saved → 06cofiggs.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/05co1120sfigg.xls\n",
      "Saved → 05co1120sfigg.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/04co1120sfigg.xls\n",
      "Saved → 04co1120sfigg.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/07cofighs.xls\n",
      "Saved → 07cofighs.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/06cofighs.xls\n",
      "Saved → 06cofighs.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/05co1120sfigh.xls\n",
      "Saved → 05co1120sfigh.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/04co1120sfigh.xls\n",
      "Saved → 04co1120sfigh.xls\n",
      "\n",
      "📄 Checking page: https://www.irs.gov/statistics/soi-tax-stats-international-business-tax-statistics\n",
      "Downloading: https://www.irs.gov/pub/irs-pdf/p5510.pdf\n",
      "Saved → p5510.pdf\n",
      "\n",
      "📄 Checking page: https://www.irs.gov/statistics/soi-tax-stats-archive\n",
      "\n",
      "📄 Checking page: https://www.irs.gov/statistics/irs-the-tax-gap\n",
      "Downloading: https://www.irs.gov/pub/irs-pdf/p5869.pdf\n",
      "Saved → p5869.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-pdf/p5870.pdf\n",
      "Saved → p5870.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-pdf/p6031.pdf\n",
      "Saved → p6031.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-prior/p5869--2023.pdf\n",
      "Saved → p5869--2023.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-prior/p5870--2023.pdf\n",
      "Saved → p5870--2023.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-pdf/p1415.pdf\n",
      "Saved → p1415.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-pdf/p5364.pdf\n",
      "Saved → p5364.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-pdf/p5365.pdf\n",
      "Saved → p5365.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-pdf/p5783.pdf\n",
      "Saved → p5783.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/24rpdistributionofindividualunderreporting.pdf\n",
      "Saved → 24rpdistributionofindividualunderreporting.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-pdf/p5784.pdf\n",
      "Saved → p5784.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-pdf/p5785.pdf\n",
      "Saved → p5785.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-pdf/p5786.pdf\n",
      "Saved → p5786.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-prior/p1415--2019.pdf\n",
      "Saved → p1415--2019.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-prior/p5364--2019.pdf\n",
      "Saved → p5364--2019.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-prior/p5365--2019.pdf\n",
      "Saved → p5365--2019.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/p1415.pdf\n",
      "Saved → p1415.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/06rastg12overvw.pdf\n",
      "Saved → 06rastg12overvw.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/06rastg12methods.pdf\n",
      "Saved → 06rastg12methods.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/06rastg12map.pdf\n",
      "Saved → 06rastg12map.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/06rastg12workppr.pdf\n",
      "Saved → 06rastg12workppr.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/12resconEstimates.pdf\n",
      "Saved → 12resconEstimates.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/12resconadvnonfile.pdf\n",
      "Saved → 12resconadvnonfile.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/01rastg07map.pdf\n",
      "Saved → 01rastg07map.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/p141596.pdf\n",
      "Saved → p141596.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/p1415e93.pdf\n",
      "Saved → p1415e93.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/p141590.pdf\n",
      "Saved → p141590.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/p7285388.pdf\n",
      "Saved → p7285388.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/12feinst.pdf\n",
      "Saved → 12feinst.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/ho.pdf\n",
      "Saved → ho.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-news/comprehensive_strategy.pdf\n",
      "Saved → comprehensive_strategy.pdf\n",
      "\n",
      "📄 Checking page: https://www.irs.gov/statistics/soi-tax-stats-irs-operations-and-budget\n",
      "\n",
      "📄 Checking page: https://www.irs.gov/statistics/soi-tax-stats-tax-exempt-bond-statistics\n",
      "Downloading: https://www.irs.gov/pub/irs-pdf/p5439.pdf\n",
      "Saved → p5439.pdf\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/22eb01gb.xls\n",
      "Saved → 22eb01gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/21eb01gb.xls\n",
      "Saved → 21eb01gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/20eb01gb.xls\n",
      "Saved → 20eb01gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/19eb01gb.xls\n",
      "Saved → 19eb01gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/18eb01gb.xls\n",
      "Saved → 18eb01gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/17eb01gb.xls\n",
      "Saved → 17eb01gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/16eb01gb.xls\n",
      "Saved → 16eb01gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/15eb01gb.xls\n",
      "Saved → 15eb01gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/14eb01gb.xls\n",
      "Saved → 14eb01gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/13bd01gb.xls\n",
      "Saved → 13bd01gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/12bd01gb.xls\n",
      "Saved → 12bd01gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/11bd01gb.xls\n",
      "Saved → 11bd01gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/10bd01gb.xls\n",
      "Saved → 10bd01gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/09bd01gb.xls\n",
      "Saved → 09bd01gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/08bd01gb.xls\n",
      "Saved → 08bd01gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/07bd01gb.xls\n",
      "Saved → 07bd01gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/06bd01gb.xls\n",
      "Saved → 06bd01gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/04bd01gb.xls\n",
      "Saved → 04bd01gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/02bd01gb.xls\n",
      "Saved → 02bd01gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/95eb01govtax.xls\n",
      "Saved → 95eb01govtax.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/22eb02gb.xls\n",
      "Saved → 22eb02gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/21eb02gb.xls\n",
      "Saved → 21eb02gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/20eb02gb.xls\n",
      "Saved → 20eb02gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/19eb02gb.xls\n",
      "Saved → 19eb02gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/18eb02gb.xls\n",
      "Saved → 18eb02gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/17eb02gb.xls\n",
      "Saved → 17eb02gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/15eb02gb.xls\n",
      "Saved → 15eb02gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/14eb02gb.xls\n",
      "Saved → 14eb02gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/13bd02gb.xls\n",
      "Saved → 13bd02gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/12bd02gb.xls\n",
      "Saved → 12bd02gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/11bd02gb.xls\n",
      "Saved → 11bd02gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/10bd02gb.xls\n",
      "Saved → 10bd02gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/09bd02gb.xls\n",
      "Saved → 09bd02gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/08bd02gb.xls\n",
      "Saved → 08bd02gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/07bd02gb.xls\n",
      "Saved → 07bd02gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/06bd02gb.xls\n",
      "Saved → 06bd02gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/05bd02gb.xls\n",
      "Saved → 05bd02gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/04bd02gb.xls\n",
      "Saved → 04bd02gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/02bd02gb.xls\n",
      "Saved → 02bd02gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/95eb02govtax.xls\n",
      "Saved → 95eb02govtax.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/22eb03gb.xls\n",
      "Saved → 22eb03gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/21eb03gb.xls\n",
      "Saved → 21eb03gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/20eb03gb.xls\n",
      "Saved → 20eb03gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/19eb03gb.xls\n",
      "Saved → 19eb03gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/18eb03gb.xls\n",
      "Saved → 18eb03gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/17eb03gb.xls\n",
      "Saved → 17eb03gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/15eb03gb.xls\n",
      "Saved → 15eb03gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/14eb03gb.xls\n",
      "Saved → 14eb03gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/13bd03gb.xls\n",
      "Saved → 13bd03gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/12bd03gb.xls\n",
      "Saved → 12bd03gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/11bd03gb.xls\n",
      "Saved → 11bd03gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/10bd03gb.xls\n",
      "Saved → 10bd03gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/09bd03gb.xls\n",
      "Saved → 09bd03gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/08bd03gb.xls\n",
      "Saved → 08bd03gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/07bd03gb.xls\n",
      "Saved → 07bd03gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/06bd03gb.xls\n",
      "Saved → 06bd03gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/05bd03gb.xls\n",
      "Saved → 05bd03gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/04bd03gb.xls\n",
      "Saved → 04bd03gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/02bd03pb.xls\n",
      "Saved → 02bd03pb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/95eb03govtax.xls\n",
      "Saved → 95eb03govtax.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/22eb04gb.xls\n",
      "Saved → 22eb04gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/21eb04gb.xls\n",
      "Saved → 21eb04gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/20eb04gb.xls\n",
      "Saved → 20eb04gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/19eb04gb.xls\n",
      "Saved → 19eb04gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/18eb04gb.xls\n",
      "Saved → 18eb04gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/17eb04gb.xls\n",
      "Saved → 17eb04gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/15eb04gb.xls\n",
      "Saved → 15eb04gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/14eb04gb.xls\n",
      "Saved → 14eb04gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/13bd04gb.xls\n",
      "Saved → 13bd04gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/12bd06pab.xls\n",
      "Saved → 12bd06pab.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/11bd04gb.xls\n",
      "Saved → 11bd04gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/10bd04gb.xls\n",
      "Saved → 10bd04gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/09bd04gb.xls\n",
      "Saved → 09bd04gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/08bd04gb.xls\n",
      "Saved → 08bd04gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/07bd04gb.xls\n",
      "Saved → 07bd04gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/06bd04gb.xls\n",
      "Saved → 06bd04gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/04bd04gb.xls\n",
      "Saved → 04bd04gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/02bd04gb.xls\n",
      "Saved → 02bd04gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/95eb04govtax.xls\n",
      "Saved → 95eb04govtax.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/22eb05gb.xls\n",
      "Saved → 22eb05gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/21eb05gb.xls\n",
      "Saved → 21eb05gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/20eb05gb.xls\n",
      "Saved → 20eb05gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/19eb05gb.xls\n",
      "Saved → 19eb05gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/18eb05gb.xls\n",
      "Saved → 18eb05gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/17eb05gb.xls\n",
      "Saved → 17eb05gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/15eb05gb.xls\n",
      "Saved → 15eb05gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/14eb05gb.xls\n",
      "Saved → 14eb05gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/13bd05gb.xls\n",
      "Saved → 13bd05gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/12bd05gb.xls\n",
      "Saved → 12bd05gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/11bd05gb.xls\n",
      "Saved → 11bd05gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/10bd05gb.xls\n",
      "Saved → 10bd05gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/09bd05gb.xls\n",
      "Saved → 09bd05gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/08bd05gb.xls\n",
      "Saved → 08bd05gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/07bd05gb.xls\n",
      "Saved → 07bd05gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/06bd05gb.xls\n",
      "Saved → 06bd05gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/05bd05gb.xls\n",
      "Saved → 05bd05gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/04bd05gb.xls\n",
      "Saved → 04bd05gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/02bd05gb.xls\n",
      "Saved → 02bd05gb.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/95eb06govtax.xls\n",
      "Saved → 95eb06govtax.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/22eb06pab.xls\n",
      "Saved → 22eb06pab.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/21eb06pab.xls\n",
      "Saved → 21eb06pab.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/20eb06pab.xls\n",
      "Saved → 20eb06pab.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/19eb06pab.xls\n",
      "Saved → 19eb06pab.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/18eb06pab.xls\n",
      "Saved → 18eb06pab.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/17eb06pab.xls\n",
      "Saved → 17eb06pab.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/16eb06pab.xls\n",
      "Saved → 16eb06pab.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/15eb06pab.xls\n",
      "Saved → 15eb06pab.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/14eb06pab.xls\n",
      "Saved → 14eb06pab.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/13bd06pab.xls\n",
      "Saved → 13bd06pab.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/12bd06pab.xls\n",
      "Saved → 12bd06pab.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/11bd06pab.xls\n",
      "Saved → 11bd06pab.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/10bd06pab.xls\n",
      "Saved → 10bd06pab.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/09bd06pab.xls\n",
      "Saved → 09bd06pab.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/08bd06pab.xls\n",
      "Saved → 08bd06pab.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/07bd06pab.xls\n",
      "Saved → 07bd06pab.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/06bd06pab.xls\n",
      "Saved → 06bd06pab.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/04bd06pab.xls\n",
      "Saved → 04bd06pab.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/02bd07ab.xls\n",
      "Saved → 02bd07ab.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/95expatb1.xls\n",
      "Saved → 95expatb1.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/22eb07pab.xls\n",
      "Saved → 22eb07pab.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/21eb07pab.xls\n",
      "Saved → 21eb07pab.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/20eb07pab.xls\n",
      "Saved → 20eb07pab.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/19eb07pab.xls\n",
      "Saved → 19eb07pab.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/18eb07pab.xls\n",
      "Saved → 18eb07pab.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/17eb07pab.xls\n",
      "Saved → 17eb07pab.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/15eb07pab.xls\n",
      "Saved → 15eb07pab.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/14eb07pab.xls\n",
      "Saved → 14eb07pab.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/13bd07pab.xls\n",
      "Saved → 13bd07pab.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/12bd07pab.xls\n",
      "Saved → 12bd07pab.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/11bd07pab.xls\n",
      "Saved → 11bd07pab.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/10bd07pab.xls\n",
      "Saved → 10bd07pab.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/09bd07pab.xls\n",
      "Saved → 09bd07pab.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/08bd07pab.xls\n",
      "Saved → 08bd07pab.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/07bd07pab.xls\n",
      "Saved → 07bd07pab.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/06bd07pab.xls\n",
      "Saved → 06bd07pab.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/05bd07pab.xls\n",
      "Saved → 05bd07pab.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/04bd07pab.xls\n",
      "Saved → 04bd07pab.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/02bd08ab.xls\n",
      "Saved → 02bd08ab.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/95expatb2.xls\n",
      "Saved → 95expatb2.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/22eb08pab.xls\n",
      "Saved → 22eb08pab.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/21eb08pab.xls\n",
      "Saved → 21eb08pab.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/20eb08pab.xls\n",
      "Saved → 20eb08pab.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/19eb08pab.xls\n",
      "Saved → 19eb08pab.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/18eb08pab.xls\n",
      "Saved → 18eb08pab.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/17eb08pab.xls\n",
      "Saved → 17eb08pab.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/15eb08pab.xls\n",
      "Saved → 15eb08pab.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/14eb08pab.xls\n",
      "Saved → 14eb08pab.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/13bd08pab.xls\n",
      "Saved → 13bd08pab.xls\n",
      "Downloading: https://www.irs.gov/pub/irs-soi/12bd08pab.xls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# -----------------------------\n",
    "# Setup\n",
    "# -----------------------------\n",
    "BASE_URL = \"https://www.irs.gov/statistics\"\n",
    "SAVE_DIR = \"IRS_Tax_Statistics\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# File types to download\n",
    "FILE_TYPES = (\".pdf\", \".xls\", \".xlsx\", \".csv\")\n",
    "\n",
    "# -----------------------------\n",
    "# Helper: download a file\n",
    "# -----------------------------\n",
    "def download_file(url, folder=SAVE_DIR):\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    filepath = os.path.join(folder, filename)\n",
    "\n",
    "    try:\n",
    "        print(f\"Downloading: {url}\")\n",
    "        r = requests.get(url, timeout=20)\n",
    "        if r.status_code == 200:\n",
    "            with open(filepath, \"wb\") as f:\n",
    "                f.write(r.content)\n",
    "            print(f\"Saved → {filename}\")\n",
    "        else:\n",
    "            print(f\"Failed ({r.status_code}): {url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {url}: {e}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Step 1: Scrape IRS Statistics Main Page\n",
    "# -----------------------------\n",
    "print(\"Fetching IRS Statistics main page...\")\n",
    "response = requests.get(BASE_URL)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Collect all sub-pages under /statistics/\n",
    "subpages = []\n",
    "\n",
    "for a in soup.find_all(\"a\", href=True):\n",
    "    href = a[\"href\"]\n",
    "    if href.startswith(\"/statistics/\"):\n",
    "        full_url = urljoin(BASE_URL, href)\n",
    "        subpages.append(full_url)\n",
    "\n",
    "subpages = list(set(subpages))  # Remove duplicates\n",
    "print(f\"Found {len(subpages)} subpages.\")\n",
    "\n",
    "# -----------------------------\n",
    "# Step 2: For each subpage, download files\n",
    "# -----------------------------\n",
    "for page in subpages:\n",
    "    print(f\"\\n📄 Checking page: {page}\")\n",
    "    try:\n",
    "        res = requests.get(page, timeout=20)\n",
    "        sp = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "        for link in sp.find_all(\"a\", href=True):\n",
    "            file_url = link[\"href\"]\n",
    "            if file_url.endswith(FILE_TYPES):\n",
    "                full_url = urljoin(page, file_url)\n",
    "                download_file(full_url)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {page}: {e}\")\n",
    "\n",
    "print(\"\\n🎉 All IRS tax statistics downloaded!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be78675-ff4a-4d40-8e11-7cbede116d32",
   "metadata": {},
   "source": [
    "#  4. COORDINATOR AGENT DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87ef0da4-e8ff-4821-bb4f-43d545464519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded Bogleheads PDF: raw_data\\bogle_slides.pdf\n",
      "Extracted text from PDF.\n",
      "✅ Scraping & extraction done. Total goals: 0\n",
      "Saved structured dataset to structured_data\\financial_goals.jsonl\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pdfplumber\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "# -----------------------------\n",
    "# Setup directories\n",
    "# -----------------------------\n",
    "os.makedirs(\"raw_data\", exist_ok=True)\n",
    "os.makedirs(\"structured_data\", exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 1: Download Bogleheads PDF\n",
    "# -----------------------------\n",
    "bogle_pdf_url = \"https://boglecenter.net/wp-content/uploads/Bogleheads-University-Full-Slide-Deck-1.pdf\"\n",
    "bogle_pdf_path = os.path.join(\"raw_data\", \"bogle_slides.pdf\")\n",
    "\n",
    "r = requests.get(bogle_pdf_url)\n",
    "with open(bogle_pdf_path, \"wb\") as f:\n",
    "    f.write(r.content)\n",
    "print(\"Downloaded Bogleheads PDF:\", bogle_pdf_path)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 2: Extract text from PDF\n",
    "# -----------------------------\n",
    "bogle_text = \"\"\n",
    "with pdfplumber.open(bogle_pdf_path) as pdf:\n",
    "    for page in pdf.pages:\n",
    "        bogle_text += page.extract_text() + \"\\n\"\n",
    "\n",
    "with open(os.path.join(\"raw_data\", \"bogle_slides.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(bogle_text)\n",
    "print(\"Extracted text from PDF.\")\n",
    "\n",
    "# -----------------------------\n",
    "# Step 3: Scrape Investopedia articles\n",
    "# -----------------------------\n",
    "investopedia_urls = [\n",
    "    \"https://www.investopedia.com/retirement-planning-strategies-5184216\",\n",
    "    \"https://www.investopedia.com/saving-for-college-5183900\"\n",
    "]\n",
    "\n",
    "investopedia_texts = []\n",
    "for url in investopedia_urls:\n",
    "    try:\n",
    "        resp = requests.get(url, timeout=10)\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        text = \" \".join(p.get_text() for p in paragraphs)\n",
    "        investopedia_texts.append({\"url\": url, \"text\": text})\n",
    "        time.sleep(1)  # polite scraping\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "\n",
    "# Save raw text\n",
    "with open(os.path.join(\"raw_data\", \"investopedia_articles.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(investopedia_texts, f, indent=2)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 4: Parse goal examples (heuristic)\n",
    "# -----------------------------\n",
    "goal_examples = []\n",
    "\n",
    "# Example: extract SMART goal pattern from Bogleheads PDF text\n",
    "pattern1 = re.compile(r'“?We will be worth \\$([\\d,\\.]+) by (.+?)”', re.IGNORECASE)\n",
    "for idx, match in enumerate(pattern1.finditer(bogle_text)):\n",
    "    goal_examples.append({\n",
    "        \"id\": f\"bogle_{idx}\",\n",
    "        \"source\": \"Bogleheads PDF\",\n",
    "        \"goal_type\": \"Net Worth Target\",\n",
    "        \"target_amount\": float(match.group(1).replace(\",\", \"\")),\n",
    "        \"target_date\": match.group(2),\n",
    "        \"allocation\": None,\n",
    "        \"risk_level\": None,\n",
    "        \"obstacles\": []\n",
    "    })\n",
    "\n",
    "# Simple heuristic: look for \"retire\", \"college\", \"home\" in Investopedia articles\n",
    "for art in investopedia_texts:\n",
    "    sentences = re.split(r'\\.|\\n', art[\"text\"])\n",
    "    for idx, s in enumerate(sentences):\n",
    "        if any(word in s.lower() for word in [\"retire\", \"college\", \"home\", \"education\"]):\n",
    "            goal_examples.append({\n",
    "                \"id\": f\"investopedia_{idx}\",\n",
    "                \"source\": art[\"url\"],\n",
    "                \"goal_type\": s.strip()[:50],  # first 50 chars as heuristic title\n",
    "                \"target_amount\": None,\n",
    "                \"target_date\": None,\n",
    "                \"allocation\": None,\n",
    "                \"risk_level\": None,\n",
    "                \"obstacles\": []\n",
    "            })\n",
    "\n",
    "# -----------------------------\n",
    "# Step 5: Save structured JSON (RAG-ready)\n",
    "# -----------------------------\n",
    "structured_file = os.path.join(\"structured_data\", \"financial_goals.jsonl\")\n",
    "with open(structured_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for goal in goal_examples:\n",
    "        f.write(json.dumps(goal) + \"\\n\")\n",
    "\n",
    "print(f\"✅ Scraping & extraction done. Total goals: {len(goal_examples)}\")\n",
    "print(f\"Saved structured dataset to {structured_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48ce7152-2129-4d5e-948e-ad64375884a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Scraping complete. 2 cases saved to case_study_structured\\financial_case_studies.jsonl\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Directories for raw and structured data\n",
    "os.makedirs(\"case_study_raw\", exist_ok=True)\n",
    "os.makedirs(\"case_study_structured\", exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 1: List of URLs to scrape\n",
    "# -----------------------------\n",
    "urls = [\n",
    "    \"https://www.investopedia.com/articles/pf/08/investing-case-study.asp\",\n",
    "    \"https://www.morningstar.com/articles/1023454/sample-investment-case-study\"\n",
    "    # Add more URLs as needed\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# Step 2: Scrape text from each page\n",
    "# -----------------------------\n",
    "case_studies = []\n",
    "for url in urls:\n",
    "    try:\n",
    "        resp = requests.get(url, timeout=10)\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        text = \" \".join(p.get_text() for p in paragraphs)\n",
    "        case_studies.append({\"url\": url, \"text\": text})\n",
    "        time.sleep(1)  # be polite\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "\n",
    "# Save raw scraped text\n",
    "with open(os.path.join(\"case_study_raw\", \"scraped_articles.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(case_studies, f, indent=2)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 3: Heuristic parsing into structured fields\n",
    "# -----------------------------\n",
    "structured_cases = []\n",
    "\n",
    "for idx, case in enumerate(case_studies):\n",
    "    text = case[\"text\"]\n",
    "\n",
    "    # Example heuristic splitting (can be improved or use NLP)\n",
    "    sentences = re.split(r'\\.|\\n', text)\n",
    "    initial_situation = sentences[0] if len(sentences) > 0 else \"\"\n",
    "    decisions_made = sentences[1] if len(sentences) > 1 else \"\"\n",
    "    alternatives = sentences[2] if len(sentences) > 2 else \"\"\n",
    "    outcomes = sentences[3] if len(sentences) > 3 else \"\"\n",
    "    lessons = sentences[4] if len(sentences) > 4 else \"\"\n",
    "\n",
    "    structured_cases.append({\n",
    "        \"id\": f\"case_{idx}\",\n",
    "        \"source_url\": case[\"url\"],\n",
    "        \"initial_situation\": initial_situation.strip(),\n",
    "        \"decisions_made\": decisions_made.strip(),\n",
    "        \"alternatives_considered\": alternatives.strip(),\n",
    "        \"outcome\": outcomes.strip(),\n",
    "        \"lessons_learned\": lessons.strip()\n",
    "    })\n",
    "\n",
    "# -----------------------------\n",
    "# Step 4: Save structured dataset\n",
    "# -----------------------------\n",
    "out_file = os.path.join(\"case_study_structured\", \"financial_case_studies.jsonl\")\n",
    "with open(out_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for case in structured_cases:\n",
    "        f.write(json.dumps(case) + \"\\n\")\n",
    "\n",
    "print(f\"✅ Scraping complete. {len(structured_cases)} cases saved to {out_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb92d9ba-aeec-4cd4-9885-ca612b3f4386",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "df = pd.read_json(\"hf://datasets/Akhil-Theerthala/PersonalFinance_v2/finance_cotr.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e8b3b4e-94f1-424d-8a74-5c33dd23208c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('finance dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cee229d-ffaa-4de0-be2e-5810d4e1cdc1",
   "metadata": {},
   "source": [
    "# State-Individual-Income-Tax-Rates-and-Brackets-2015-2024_Tax_Foundation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fda07fc0-eb68-4f86-a45d-488a2581b6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sheet: 2024\n",
      "Processing sheet: 2023\n",
      "Processing sheet: 2022\n",
      "Processing sheet: 2021\n",
      "Processing sheet: 2020\n",
      "Processing sheet: 2019\n",
      "Processing sheet: 2018\n",
      "Processing sheet: 2017\n",
      "Processing sheet: 2016\n",
      "Processing sheet: 2015\n",
      "Scleaned_tax_with_notes.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "file_path = \"C:\\\\Users\\\\itsam\\\\Downloads\\\\agents datset\\\\TAX OPTIMIZATION AGENT DATASETS\\\\State-Individual-Income-Tax-Rates-and-Brackets-2015-2024_Tax_Foundation.xlsx\"\n",
    "\n",
    "# Load all sheets\n",
    "sheets = pd.read_excel(file_path, sheet_name=None)\n",
    "\n",
    "cleaned_frames = []\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Function — Extract footnotes from the bottom\n",
    "# -----------------------------\n",
    "\n",
    "def extract_footnotes(text_block):\n",
    "    \"\"\"\n",
    "    Extracts footnotes like:\n",
    "        (a) explanation...\n",
    "        (b) explanation...\n",
    "    Returns dictionary:\n",
    "        {'a': 'Local income taxes...', 'b': 'These states allow...', ...}\n",
    "    \"\"\"\n",
    "\n",
    "    footnote_pattern = r\"\\(([a-z])\\)\\s*(.*?)\\s*(?=\\([a-z]\\)|$)\"\n",
    "\n",
    "    notes = dict(re.findall(footnote_pattern, text_block, flags=re.S))\n",
    "    return notes\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Main sheet processor\n",
    "# -----------------------------\n",
    "\n",
    "def process_sheet(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Separate data rows vs informational text\n",
    "    footer_rows = df[df.apply(lambda r: r.astype(str).str.contains(r\"\\([a-z]\\)\").any(), axis=1)]\n",
    "\n",
    "    # Join footer rows into one long text block\n",
    "    footer_text = \" \".join(\" \".join(row.astype(str).values) for idx, row in footer_rows.iterrows())\n",
    "\n",
    "    # Extract footnote mapping\n",
    "    footnotes = extract_footnotes(footer_text)\n",
    "\n",
    "    # Drop the footer rows from main data\n",
    "    df = df.drop(footer_rows.index)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Extract footnote letters from state column\n",
    "    # -----------------------------\n",
    "\n",
    "    if \"State\" not in df.columns:\n",
    "        # Try to find state column by heuristic\n",
    "        for col in df.columns:\n",
    "            if df[col].astype(str).str.contains(r\"Ala\\.|Alaska|N\\.Y\\.\").any():\n",
    "                df.rename(columns={col: \"State\"}, inplace=True)\n",
    "                break\n",
    "\n",
    "    # Find things like \"(a, b, c)\"\n",
    "    df[\"footnotes\"] = df[\"State\"].astype(str).str.extract(r\"\\((.*?)\\)\", expand=False)\n",
    "\n",
    "    # Remove \"(a, b, c)\" from State name\n",
    "    df[\"State\"] = df[\"State\"].astype(str).str.replace(r\"\\(.*?\\)\", \"\", regex=True).str.strip()\n",
    "\n",
    "    # -----------------------------\n",
    "    # Expand footnotes into columns\n",
    "    # -----------------------------\n",
    "\n",
    "    for code, explanation in footnotes.items():\n",
    "        col_name = f\"note_{code}\"\n",
    "        df[col_name] = df[\"footnotes\"].apply(\n",
    "            lambda x: explanation if (isinstance(x, str) and code in x) else None\n",
    "        )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Apply to all sheets\n",
    "# -----------------------------\n",
    "\n",
    "for sheet_name, df in sheets.items():\n",
    "    print(f\"Processing sheet: {sheet_name}\")\n",
    "    cleaned = process_sheet(df)\n",
    "    cleaned[\"source_sheet\"] = sheet_name\n",
    "    cleaned_frames.append(cleaned)\n",
    "\n",
    "combined_df = pd.concat(cleaned_frames, ignore_index=True)\n",
    "\n",
    "combined_df.to_csv(\"cleaned_tax_with_notes.csv\", index=False)\n",
    "\n",
    "print(\"Scleaned_tax_with_notes.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47b327c8-a885-46f8-b0aa-9ee38b0c30af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\itsam\\AppData\\Local\\Temp\\ipykernel_8004\\3936340586.py:41: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(remove_info_text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final cleaned file saved to: final_tax_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# ---------------------------------------\n",
    "# 1. Load the saved cleaned CSV file\n",
    "# ---------------------------------------\n",
    "file_path = \"cleaned_tax_with_notes.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 2. Remove informational/explanatory text from columns\n",
    "# -----------------------------------------------------\n",
    "\n",
    "def remove_info_text(cell):\n",
    "    \"\"\"\n",
    "    Removes long explanatory text or footnotes from any cell.\n",
    "    Rules:\n",
    "    - Remove text with long sentences\n",
    "    - Remove extra symbols\n",
    "    \"\"\"\n",
    "    if pd.isna(cell):\n",
    "        return cell\n",
    "    \n",
    "    text = str(cell)\n",
    "\n",
    "    # Remove long multi-sentence information\n",
    "    if len(text.split()) > 25:  # too long => probably explanation\n",
    "        return None\n",
    "    \n",
    "    # Remove footnote patterns like (a), (b, c)\n",
    "    text = re.sub(r\"\\(.*?\\)\", \"\", text)\n",
    "\n",
    "    # Remove excessive hyphens, quotes, URLs, and reference text\n",
    "    text = re.sub(r\"http[s]?:\\/\\/\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[\\\"“”]\", \"\", text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "df = df.applymap(remove_info_text)\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 3. Drop any remaining rows that are all NaN\n",
    "# -----------------------------------------------------\n",
    "df = df.dropna(how='all')\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 4. Drop rows where essential columns became empty\n",
    "# -----------------------------------------------------\n",
    "if \"State\" in df.columns:\n",
    "    df = df[df[\"State\"].notna()]\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 5. Save cleaned final dataset\n",
    "# -----------------------------------------------------\n",
    "output_path = \"final_tax_cleaned.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"Final cleaned file saved to:\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6637bc8b-1c44-464d-ac85-65a04093dcc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\itsam\\AppData\\Local\\Temp\\ipykernel_17156\\3138123710.py:18: DtypeWarning: Columns (38,39,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All CSV files have been combined into 'combined_data.csv'!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "import re\n",
    "\n",
    "# Path to your folder containing CSV files\n",
    "folder_path = \"C:\\\\Users\\\\itsam\\\\Downloads\\\\archive\"  # update as needed\n",
    "\n",
    "# Get a list of all CSV files in the folder\n",
    "csv_files = glob(os.path.join(folder_path, \"*.csv\"))\n",
    "\n",
    "# Initialize a list to store dataframes\n",
    "dfs = []\n",
    "\n",
    "# Loop through files and read them\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "        \n",
    "        # Extract the first 4-digit number in filename as year\n",
    "        match = re.search(r'\\b(20\\d{2}|19\\d{2})\\b', os.path.basename(file))\n",
    "        if match:\n",
    "            year = int(match.group(0))\n",
    "        else:\n",
    "            year = None  # If no year found, leave as None\n",
    "        \n",
    "        df['year'] = year\n",
    "        dfs.append(df)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "# Concatenate all dataframes\n",
    "if dfs:\n",
    "    final_df = pd.concat(dfs, ignore_index=True)\n",
    "    # Save to a single CSV\n",
    "    final_df.to_csv(os.path.join(folder_path, \"combined_data.csv\"), index=False)\n",
    "    print(\"All CSV files have been combined into 'combined_data.csv'!\")\n",
    "else:\n",
    "    print(\"No CSV files were read successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23f33e45-455e-42de-8ebf-30ad56423352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find kaggle.json. Make sure it's located in C:\\Users\\itsam\\.kaggle. Or use the environment method. See setup instructions at https://github.com/Kaggle/kaggle-api/\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkaggle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkaggle_api_extended\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KaggleApi\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mzipfile\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\kaggle\\__init__.py:10\u001b[0m\n\u001b[0;32m      8\u001b[0m enable_oauth \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKAGGLE_ENABLE_OAUTH\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m api \u001b[38;5;241m=\u001b[39m KaggleApi(enable_oauth\u001b[38;5;241m=\u001b[39menable_oauth)\n\u001b[1;32m---> 10\u001b[0m api\u001b[38;5;241m.\u001b[39mauthenticate()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\kaggle\\api\\kaggle_api_extended.py:685\u001b[0m, in \u001b[0;36mKaggleApi.authenticate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    680\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. Make sure it\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms located in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. Or use the environment method. See setup\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    682\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m instructions at\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    683\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m https://github.com/Kaggle/kaggle-api/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_file, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_dir)\n\u001b[0;32m    684\u001b[0m     )\n\u001b[1;32m--> 685\u001b[0m exit(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Initialize API\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "# Dataset info\n",
    "dataset = 'irs/irs-990'\n",
    "download_path = './irs-990.zip'   # where zip will be saved\n",
    "extract_path = './irs-990'        # folder where files will be extracted\n",
    "\n",
    "# Download dataset\n",
    "api.dataset_download_files(dataset, path='.', unzip=False)\n",
    "\n",
    "# Extract zip\n",
    "with zipfile.ZipFile(download_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n",
    "print(f\"Dataset downloaded and extracted to {extract_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9579bccc-0133-453f-8864-945aac64329c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They're both strings/string literals.\n"
     ]
    }
   ],
   "source": [
    "print(\"They're both strings/string literals.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80989f0-bf9a-4a77-aed0-441a68a126b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
